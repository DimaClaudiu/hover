{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hover - API Documentation This site hosts the more technical details and considerations of the Hover package. For feature demonstrations and quick examples, check out the project homepage .","title":"Home"},{"location":"#hover-api-documentation","text":"This site hosts the more technical details and considerations of the Hover package. For feature demonstrations and quick examples, check out the project homepage .","title":"Hover - API Documentation"},{"location":"reference/core-dataset/","text":"hover.core.dataset.SupervisableDataset Feature-agnostic class for a dataset open to supervision. Raw -- piecewise annoatation -> Gold -> Dev/Test Raw -- batch annotation -> Noisy -> Train Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that - the DataFrame form supports most kinds of operations; - the list-of-dicts form could be useful for manipulations outside the scope of pandas; - synchronization between the two forms should be called sparingly. __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = 'feature' , label_key = 'label' ) special Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. param raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. param train_dictl: a list of dicts holding the batch-annotated noisy train set. param dev_dictl: a list of dicts holding the gold dev set. param test_dictl: a list of dicts holding the gold test set. param feature_key: key in each piece of dict mapping to the feature. param label_key: key in each piece of dict mapping to the ground truth in STRING form. Source code in hover/core/dataset.py def __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. - param raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. - param train_dictl: a list of dicts holding the batch-annotated noisy train set. - param dev_dictl: a list of dicts holding the gold dev set. - param test_dictl: a list of dicts holding the gold test set. - param feature_key: key in each piece of dict mapping to the feature. - param label_key: key in each piece of dict mapping to the ground truth in STRING form. \"\"\" def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] self . dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } self . synchronize_dictl_to_df () self . df_deduplicate () self . synchronize_df_to_dictl () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_pop_table () compute_2d_embedding ( self , vectorizer , method , ** kwargs ) Get embeddings in the xy-plane and return the reducer. Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" Get embeddings in the xy-plane and return the reducer. \"\"\" from hover.core.representation.reduction import DimensionalityReducer # prepare input vectors to manifold learning subset = [ \"raw\" , \"train\" , \"dev\" ] fit_inp = [] for _key in subset : _df = self . dfs [ _key ] if _df . empty : continue fit_inp += _df [ self . __class__ . FEATURE_KEY ] . tolist () fit_arr = np . array ([ vectorizer ( _inp ) for _inp in tqdm ( fit_inp )]) # initialize and fit manifold learning reducer reducer = DimensionalityReducer ( fit_arr ) embedding = reducer . fit_transform ( method , ** kwargs ) # assign x and y coordinates to dataset start_idx = 0 for _key in subset : _df = self . dfs [ _key ] _length = _df . shape [ 0 ] _df [ \"x\" ] = pd . Series ( embedding [ start_idx : ( start_idx + _length ), 0 ]) _df [ \"y\" ] = pd . Series ( embedding [ start_idx : ( start_idx + _length ), 1 ]) start_idx += _length return reducer df_deduplicate ( self ) Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" Cross-deduplicate data entries by feature between subsets. \"\"\" # for data entry accounting before , after = dict (), dict () # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in self . __class__ . ORDERED_SUBSET : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in self . __class__ . ORDERED_SUBSET ], axis = 0 , sort = False , ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"first\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in self . __class__ . ORDERED_SUBSET : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] console . print ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" , style = \"blue\" ) loader ( self , key , vectorizer , batch_size = 64 , smoothing_coeff = 0.0 ) Prepare a Torch Dataloader for training or evaluation. param key(str): the subset of dataset to use. param vectorizer(callable): callable that turns a string into a vector. param smoothing_coeff: the smoothing coeffient for soft labels. type smoothing_coeff: float Source code in hover/core/dataset.py def loader ( self , key , vectorizer , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" Prepare a Torch Dataloader for training or evaluation. - param key(str): the subset of dataset to use. - param vectorizer(callable): callable that turns a string into a vector. - param smoothing_coeff: the smoothing coeffient for soft labels. - type smoothing_coeff: float \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import vector_dataloader , one_hot , label_smoothing # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () features = df [ self . __class__ . FEATURE_KEY ] . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) console . print ( f \"Preparing { key } input vectors...\" , style = \"blue\" ) input_vectors = [ vectorizer ( _f ) for _f in tqdm ( features )] if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) console . print ( f \"Preparing { key } data loader...\" , style = \"blue\" ) loader = vector_dataloader ( input_vectors , output_vectors , batch_size = batch_size ) console . print ( f \"Prepared { key } loader consisting of { len ( features ) } examples with batch size { batch_size } \" , style = \"green\" , ) return loader setup_label_coding ( self ) Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. Source code in hover/core/dataset.py def setup_label_coding ( self ): \"\"\" Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. \"\"\" all_labels = set () for _key in self . __class__ . ORDERED_SUBSET [: - 1 ]: _df = self . dfs [ _key ] if _df . empty : continue assert \"label\" in _df . columns _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} console . print ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" , style = \"green\" , ) self . validate_labels () setup_widgets ( self ) Critical widgets for interactive data management. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" Critical widgets for interactive data management. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ \"train\" , \"dev\" , \"test\" ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) def callback_dedup (): self . deduplicate () self . dedup_trigger . on_click ( callback_dedup ) subscribe_data_commit ( self , explorer , subset_mapping ) Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. \"\"\" if self . data_committer . subscribed_events : console . print ( f \"Attempting subscribe_data_commit: found existing callback, and there should only be one such callback\" , style = \"red\" , ) return def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item select_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : console . print ( \"Attempting data commit: did not select any data points.\" , style = \"yellow\" , ) return selected_slice = self . dfs [ sub_k ] . at [ selected_idx ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], selected_slice ], axis = 0 , sort = False ) self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"first\" , inplace = True ) console . print ( f \"Committed { selected_slice . shape [ 0 ] } entries from [ { sub_k } ] to [ { sub_to } ].\" , style = \"blue\" , ) self . data_committer . on_click ( callback_commit ) console . print ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" , style = \"green\" , ) subscribe_update_push ( self , explorer , subset_mapping ) Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" assert isinstance ( explorer , BokehForLabeledText ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) console . print ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" , style = \"green\" , ) synchronize_df_to_dictl ( self ) Re-make lists of dictionaries from dataframes. Source code in hover/core/dataset.py def synchronize_df_to_dictl ( self ): \"\"\" Re-make lists of dictionaries from dataframes. \"\"\" self . dictls = dict () for _key , _df in self . dfs . items (): self . dictls [ _key ] = _df . to_dict ( orient = \"records\" ) synchronize_dictl_to_df ( self ) Re-make dataframes from lists of dictionaries. Source code in hover/core/dataset.py def synchronize_dictl_to_df ( self ): \"\"\" Re-make dataframes from lists of dictionaries. \"\"\" self . dfs = dict () for _key , _dictl in self . dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df validate_labels ( self , raise_exception = True ) Check that every label is in the encoder. Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" Check that every label is in the encoder. \"\"\" for _key in self . __class__ . ORDERED_SUBSET [: - 1 ]: _df = self . dfs [ _key ] _invalid_indices = None if _df . empty : continue assert \"label\" in _df . columns _mask = _df [ \"label\" ] . apply ( lambda x : x in self . label_encoder ) _invalid_indices = np . where ( _mask == False )[ 0 ] . tolist () if _invalid_indices : console . print ( f \"Subset [ { _key } ] has invalid labels:\" ) console . print ({ _df . loc [ _invalid_indices ]}) if raise_exception : raise ValueError ( \"invalid labels\" ) view ( self ) Defines the layout of bokeh models. Source code in hover/core/dataset.py def view ( self ): \"\"\" Defines the layout of bokeh models. \"\"\" return column ( row ( self . update_pusher , self . data_committer , self . dedup_trigger ), self . pop_table , ) hover.core.dataset.SupervisableTextDataset Can add text-specific methods.","title":"hover.core.dataset"},{"location":"reference/core-dataset/#hovercoredatasetsupervisabledataset","text":"","title":"hover.core.dataset.SupervisableDataset"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset","text":"Feature-agnostic class for a dataset open to supervision. Raw -- piecewise annoatation -> Gold -> Dev/Test Raw -- batch annotation -> Noisy -> Train Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that - the DataFrame form supports most kinds of operations; - the list-of-dicts form could be useful for manipulations outside the scope of pandas; - synchronization between the two forms should be called sparingly.","title":"hover.core.dataset.SupervisableDataset"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.__init__","text":"Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. param raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. param train_dictl: a list of dicts holding the batch-annotated noisy train set. param dev_dictl: a list of dicts holding the gold dev set. param test_dictl: a list of dicts holding the gold test set. param feature_key: key in each piece of dict mapping to the feature. param label_key: key in each piece of dict mapping to the ground truth in STRING form. Source code in hover/core/dataset.py def __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. - param raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. - param train_dictl: a list of dicts holding the batch-annotated noisy train set. - param dev_dictl: a list of dicts holding the gold dev set. - param test_dictl: a list of dicts holding the gold test set. - param feature_key: key in each piece of dict mapping to the feature. - param label_key: key in each piece of dict mapping to the ground truth in STRING form. \"\"\" def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] self . dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } self . synchronize_dictl_to_df () self . df_deduplicate () self . synchronize_df_to_dictl () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_pop_table ()","title":"__init__()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_2d_embedding","text":"Get embeddings in the xy-plane and return the reducer. Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" Get embeddings in the xy-plane and return the reducer. \"\"\" from hover.core.representation.reduction import DimensionalityReducer # prepare input vectors to manifold learning subset = [ \"raw\" , \"train\" , \"dev\" ] fit_inp = [] for _key in subset : _df = self . dfs [ _key ] if _df . empty : continue fit_inp += _df [ self . __class__ . FEATURE_KEY ] . tolist () fit_arr = np . array ([ vectorizer ( _inp ) for _inp in tqdm ( fit_inp )]) # initialize and fit manifold learning reducer reducer = DimensionalityReducer ( fit_arr ) embedding = reducer . fit_transform ( method , ** kwargs ) # assign x and y coordinates to dataset start_idx = 0 for _key in subset : _df = self . dfs [ _key ] _length = _df . shape [ 0 ] _df [ \"x\" ] = pd . Series ( embedding [ start_idx : ( start_idx + _length ), 0 ]) _df [ \"y\" ] = pd . Series ( embedding [ start_idx : ( start_idx + _length ), 1 ]) start_idx += _length return reducer","title":"compute_2d_embedding()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.df_deduplicate","text":"Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" Cross-deduplicate data entries by feature between subsets. \"\"\" # for data entry accounting before , after = dict (), dict () # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in self . __class__ . ORDERED_SUBSET : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in self . __class__ . ORDERED_SUBSET ], axis = 0 , sort = False , ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"first\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in self . __class__ . ORDERED_SUBSET : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] console . print ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" , style = \"blue\" )","title":"df_deduplicate()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.loader","text":"Prepare a Torch Dataloader for training or evaluation. param key(str): the subset of dataset to use. param vectorizer(callable): callable that turns a string into a vector. param smoothing_coeff: the smoothing coeffient for soft labels. type smoothing_coeff: float Source code in hover/core/dataset.py def loader ( self , key , vectorizer , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" Prepare a Torch Dataloader for training or evaluation. - param key(str): the subset of dataset to use. - param vectorizer(callable): callable that turns a string into a vector. - param smoothing_coeff: the smoothing coeffient for soft labels. - type smoothing_coeff: float \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import vector_dataloader , one_hot , label_smoothing # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () features = df [ self . __class__ . FEATURE_KEY ] . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) console . print ( f \"Preparing { key } input vectors...\" , style = \"blue\" ) input_vectors = [ vectorizer ( _f ) for _f in tqdm ( features )] if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) console . print ( f \"Preparing { key } data loader...\" , style = \"blue\" ) loader = vector_dataloader ( input_vectors , output_vectors , batch_size = batch_size ) console . print ( f \"Prepared { key } loader consisting of { len ( features ) } examples with batch size { batch_size } \" , style = \"green\" , ) return loader","title":"loader()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_label_coding","text":"Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. Source code in hover/core/dataset.py def setup_label_coding ( self ): \"\"\" Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. \"\"\" all_labels = set () for _key in self . __class__ . ORDERED_SUBSET [: - 1 ]: _df = self . dfs [ _key ] if _df . empty : continue assert \"label\" in _df . columns _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} console . print ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" , style = \"green\" , ) self . validate_labels ()","title":"setup_label_coding()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_widgets","text":"Critical widgets for interactive data management. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" Critical widgets for interactive data management. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ \"train\" , \"dev\" , \"test\" ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) def callback_dedup (): self . deduplicate () self . dedup_trigger . on_click ( callback_dedup )","title":"setup_widgets()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_data_commit","text":"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. \"\"\" if self . data_committer . subscribed_events : console . print ( f \"Attempting subscribe_data_commit: found existing callback, and there should only be one such callback\" , style = \"red\" , ) return def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item select_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : console . print ( \"Attempting data commit: did not select any data points.\" , style = \"yellow\" , ) return selected_slice = self . dfs [ sub_k ] . at [ selected_idx ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], selected_slice ], axis = 0 , sort = False ) self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"first\" , inplace = True ) console . print ( f \"Committed { selected_slice . shape [ 0 ] } entries from [ { sub_k } ] to [ { sub_to } ].\" , style = \"blue\" , ) self . data_committer . on_click ( callback_commit ) console . print ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" , style = \"green\" , )","title":"subscribe_data_commit()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_update_push","text":"Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" assert isinstance ( explorer , BokehForLabeledText ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) console . print ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" , style = \"green\" , )","title":"subscribe_update_push()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_df_to_dictl","text":"Re-make lists of dictionaries from dataframes. Source code in hover/core/dataset.py def synchronize_df_to_dictl ( self ): \"\"\" Re-make lists of dictionaries from dataframes. \"\"\" self . dictls = dict () for _key , _df in self . dfs . items (): self . dictls [ _key ] = _df . to_dict ( orient = \"records\" )","title":"synchronize_df_to_dictl()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_dictl_to_df","text":"Re-make dataframes from lists of dictionaries. Source code in hover/core/dataset.py def synchronize_dictl_to_df ( self ): \"\"\" Re-make dataframes from lists of dictionaries. \"\"\" self . dfs = dict () for _key , _dictl in self . dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df","title":"synchronize_dictl_to_df()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.validate_labels","text":"Check that every label is in the encoder. Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" Check that every label is in the encoder. \"\"\" for _key in self . __class__ . ORDERED_SUBSET [: - 1 ]: _df = self . dfs [ _key ] _invalid_indices = None if _df . empty : continue assert \"label\" in _df . columns _mask = _df [ \"label\" ] . apply ( lambda x : x in self . label_encoder ) _invalid_indices = np . where ( _mask == False )[ 0 ] . tolist () if _invalid_indices : console . print ( f \"Subset [ { _key } ] has invalid labels:\" ) console . print ({ _df . loc [ _invalid_indices ]}) if raise_exception : raise ValueError ( \"invalid labels\" )","title":"validate_labels()"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableDataset.view","text":"Defines the layout of bokeh models. Source code in hover/core/dataset.py def view ( self ): \"\"\" Defines the layout of bokeh models. \"\"\" return column ( row ( self . update_pusher , self . data_committer , self . dedup_trigger ), self . pop_table , )","title":"view()"},{"location":"reference/core-dataset/#hovercoredatasetsupervisabletextdataset","text":"","title":"hover.core.dataset.SupervisableTextDataset"},{"location":"reference/core-dataset/#hover.core.dataset.SupervisableTextDataset","text":"Can add text-specific methods.","title":"hover.core.dataset.SupervisableTextDataset"},{"location":"reference/core-explorer/","text":"hover.core.explorer.BokehForLabeledText Base class that keeps template explorer settings. Assumes: in supplied dataframes (always) text data in a text column (always) xy coordinates in x and y columns (always) an index for the rows (likely) classification label in a label column Does not assume: what the explorer serves to do. __init__ ( self , df_dict , ** kwargs ) special Operations shared by all child classes. settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults create widgets that child classes can override create data sources the correspond to class-specific data subsets. activate builtin search callbacks depending on the child class. create a (typically) blank figure under such settings Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" Operations shared by all child classes. - settle the figure settings by using child class defaults & kwargs overrides - settle the glyph settings by using child class defaults - create widgets that child classes can override - create data sources the correspond to class-specific data subsets. - activate builtin search callbacks depending on the child class. - create a (typically) blank figure under such settings \"\"\" logger . divider ( f \"Initializing { self . __class__ . __name__ } \" ) self . figure_kwargs = self . __class__ . DEFAULT_FIGURE_KWARGS . copy () self . figure_kwargs . update ( kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . DATA_KEY_TO_KWARGS . items () } self . _setup_widgets () self . _setup_dfs ( df_dict ) self . _setup_sources () self . _activate_search_builtin () self . figure = figure ( ** self . figure_kwargs ) self . reset_figure () _activate_search_builtin ( self ) private Typically called once during initialization. Highlight positive search results and mute negative search results. Note that this is a template method which heavily depends on class attributes. Source code in hover/core/explorer.py def _activate_search_builtin ( self ): \"\"\" Typically called once during initialization. Highlight positive search results and mute negative search results. Note that this is a template method which heavily depends on class attributes. \"\"\" logger . info ( \"Activating built-in search\" ) for _key , _dict in self . __class__ . DATA_KEY_TO_KWARGS . items (): for _flag , _params in _dict [ \"search\" ] . items (): logger . info ( f \"Activated { _flag } on subset { _key } to respond to the search widgets.\" ) self . glyph_kwargs [ _key ] = self . activate_search ( self . sources [ _key ], self . glyph_kwargs [ _key ], altered_param = _params ) _layout_widgets ( self ) private Define the layout of widgets. Source code in hover/core/explorer.py def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" return column ( self . search_pos , self . search_neg , self . data_key_button_group ) _prelink_check ( self , other ) private Sanity check before linking two explorers. Source code in hover/core/explorer.py def _prelink_check ( self , other ): \"\"\" Sanity check before linking two explorers. \"\"\" assert other is not self , \"Self-loops are fordidden\" assert isinstance ( other , BokehForLabeledText ), \"Must link to BokehForLabelText\" _setup_dfs ( self , df_dict , copy = False ) private Check and store DataFrames BY REFERENCE BY DEFAULT. Intended to be extended in child classes for pre/post processing. Source code in hover/core/explorer.py def _setup_dfs ( self , df_dict , copy = False ): \"\"\" Check and store DataFrames BY REFERENCE BY DEFAULT. Intended to be extended in child classes for pre/post processing. \"\"\" logger . info ( \"Setting up dfs\" ) supplied_keys = set ( df_dict . keys ()) expected_keys = set ( self . __class__ . DATA_KEY_TO_KWARGS . keys ()) supplied_not_expected = supplied_keys . difference ( expected_keys ) expected_not_supplied = expected_keys . difference ( supplied_keys ) for _key in supplied_not_expected : logger . warn ( f \" { self . __class__ . __name__ } .__init__(): got unexpected df key { _key } \" ) for _key in expected_not_supplied : logger . warn ( f \" { self . __class__ . __name__ } .__init__(): missing expected df key { _key } \" ) self . dfs = { _key : ( _df . copy () if copy else _df ) for _key , _df in df_dict . items () } _setup_sources ( self ) private Create (NOT UPDATE) ColumnDataSource objects. Intended to be extended in child classes for pre/post processing. Source code in hover/core/explorer.py def _setup_sources ( self ): \"\"\" Create (NOT UPDATE) ColumnDataSource objects. Intended to be extended in child classes for pre/post processing. \"\"\" logger . info ( \"Setting up sources\" ) self . sources = { _key : ColumnDataSource ( _df ) for _key , _df in self . dfs . items ()} _setup_widgets ( self ) private Prepare widgets for interactive functionality. Create positive/negative text search boxes. Source code in hover/core/explorer.py def _setup_widgets ( self ): \"\"\" Prepare widgets for interactive functionality. Create positive/negative text search boxes. \"\"\" from bokeh.models import TextInput , CheckboxButtonGroup # set up text search widgets, without assigning callbacks yet # to provide more flexibility with callbacks logger . info ( \"Setting up widgets\" ) self . search_pos = TextInput ( title = \"Text contains (plain text, or /pattern/flag for regex):\" , width_policy = \"fit\" , height_policy = \"fit\" , ) self . search_neg = TextInput ( title = \"Text does not contain:\" , width_policy = \"fit\" , height_policy = \"fit\" ) # set up subset display toggles which do have clearly defined callbacks data_keys = list ( self . __class__ . DATA_KEY_TO_KWARGS . keys ()) self . data_key_button_group = CheckboxButtonGroup ( labels = data_keys , active = list ( range ( len ( data_keys ))) ) def update_data_key_display ( active ): visible_keys = { self . data_key_button_group . labels [ idx ] for idx in active } for _renderer in self . figure . renderers : # if the renderer has a name \"on the list\", update its visibility if _renderer . name in self . __class__ . DATA_KEY_TO_KWARGS . keys (): _renderer . visible = _renderer . name in visible_keys # store the callback (useful, for example, during automated tests) and link it self . update_data_key_display = update_data_key_display self . data_key_button_group . on_click ( self . update_data_key_display ) _update_sources ( self ) private Update the sources with the corresponding dfs. Note that it seems mandatory to re-activate the search widgets. This is because the source loses plotting kwargs. Source code in hover/core/explorer.py def _update_sources ( self ): \"\"\" Update the sources with the corresponding dfs. Note that it seems mandatory to re-activate the search widgets. This is because the source loses plotting kwargs. \"\"\" for _key in self . __class__ . DATA_KEY_TO_KWARGS . keys (): self . sources [ _key ] . data = self . dfs [ _key ] self . _activate_search_builtin () activate_search ( self , source , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Source code in hover/core/explorer.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. \"\"\" assert isinstance ( source , ColumnDataSource ) assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () param_key , param_pos , param_neg , param_default = altered_param num_points = len ( source . data [ \"text\" ]) default_param_list = [ param_default ] * num_points source . add ( default_param_list , f \" { param_key } \" ) updated_kwargs [ param_key ] = param_key search_callback = CustomJS ( args = { \"source\" : source , \"key_pos\" : self . search_pos , \"key_neg\" : self . search_neg , \"param_pos\" : param_pos , \"param_neg\" : param_neg , \"param_default\" : param_default , }, code = f \"\"\" const data = source.data; const text = data['text']; var arr = data[' { param_key } ']; \"\"\" + \"\"\" var search_pos = key_pos.value; var search_neg = key_neg.value; var valid_pos = (search_pos.length > 0); var valid_neg = (search_neg.length > 0); function determineAttr(candidate) { var score = 0; if (valid_pos) { if (candidate.search(search_pos) >= 0) { score += 1; } else { score -= 2; } }; if (valid_neg) { if (candidate.search(search_neg) < 0) { score += 1; } else { score -= 2; } }; if (score > 0) { return param_pos; } else if (score < 0) { return param_neg; } else {return param_default;} } function toRegex(search_key) { var match = search_key.match(new RegExp('^/(.*?)/([gimy]*)$')); if (match) { return new RegExp(match[1], match[2]); } else { return search_key; } } if (valid_pos) {search_pos = toRegex(search_pos);} if (valid_neg) {search_neg = toRegex(search_neg);} for (var i = 0; i < arr.length; i++) { arr[i] = determineAttr(text[i]); } source.change.emit() \"\"\" , ) self . search_pos . js_on_change ( \"value\" , search_callback ) self . search_neg . js_on_change ( \"value\" , search_callback ) return updated_kwargs from_dataset ( dataset , subset_mapping , * args , ** kwargs ) classmethod Construct from a SupervisableDataset. Source code in hover/core/explorer.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" Construct from a SupervisableDataset. \"\"\" assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} return cls ( df_dict , * args , ** kwargs ) link_selection ( self , key , other , other_key ) Sync the selected indices between specified sources. Source code in hover/core/explorer.py def link_selection ( self , key , other , other_key ): \"\"\" Sync the selected indices between specified sources. \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] sl . selected . js_link ( \"indices\" , sr . selected , \"indices\" ) sr . selected . js_link ( \"indices\" , sl . selected , \"indices\" ) link_xy_range ( self , other ) Sync plotting ranges on the xy-plane. Source code in hover/core/explorer.py def link_xy_range ( self , other ): \"\"\" Sync plotting ranges on the xy-plane. \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr ) reset_figure ( self ) Start over on the figure. Source code in hover/core/explorer.py def reset_figure ( self ): \"\"\"Start over on the figure.\"\"\" logger . info ( \"Resetting figure\" ) self . figure . renderers . clear () view ( self ) Define the layout of the whole explorer. Source code in hover/core/explorer.py def view ( self ): \"\"\"Define the layout of the whole explorer.\"\"\" return column ( self . _layout_widgets (), self . figure ) hover.core.explorer.BokehCorpusExplorer Plot unlabeled, 2D-vectorized text data points in a corpus. Features: the search widgets will highlight the results through a change of color, which is arguably the best visual effect. __init__ ( self , df_dict , ** kwargs ) special Requires the input dataframe to contain: (1) \"x\" and \"y\" columns for coordinates; (2) a \"text\" column for data point tooltips. Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" Requires the input dataframe to contain: (1) \"x\" and \"y\" columns for coordinates; (2) a \"text\" column for data point tooltips. \"\"\" super () . __init__ ( df_dict , ** kwargs ) plot ( self , * args , ** kwargs ) (Re)-plot the corpus. Called just once per instance most of the time. Source code in hover/core/explorer.py def plot ( self , * args , ** kwargs ): \"\"\" (Re)-plot the corpus. Called just once per instance most of the time. \"\"\" self . figure . circle ( \"x\" , \"y\" , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) hover.core.explorer.BokehCorpusAnnotator Annoate text data points via callbacks. Features: alter values in the 'label' column through the widgets. SERVER ONLY : only works in a setting that allows Python callbacks. __init__ ( self , df_dict , ** kwargs ) special Conceptually the same as the parent method. Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\"Conceptually the same as the parent method.\"\"\" super () . __init__ ( df_dict , ** kwargs ) _layout_widgets ( self ) private Define the layout of widgets. Source code in hover/core/explorer.py def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . search_pos , self . search_neg ), row ( self . data_key_button_group ), row ( self . annotator_input , self . annotator_apply , self . annotator_export ), ) return column ( * layout_rows ) _setup_widgets ( self ) private Create annotator widgets and assign Python callbacks. Source code in hover/core/explorer.py def _setup_widgets ( self ): \"\"\" Create annotator widgets and assign Python callbacks. \"\"\" from bokeh.models import TextInput , Button , Dropdown super () . _setup_widgets () self . annotator_input = TextInput ( title = \"Label:\" ) self . annotator_apply = Button ( label = \"Apply\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . annotator_export = Dropdown ( label = \"Export\" , button_type = \"warning\" , menu = [ \"Excel\" , \"CSV\" , \"JSON\" , \"pickle\" ], height_policy = \"fit\" , width_policy = \"min\" , ) def callback_apply (): \"\"\" A callback on clicking the 'self.annotator_apply' button. Update labels in the source. \"\"\" label = self . annotator_input . value selected_idx = self . sources [ \"raw\" ] . selected . indices if not selected_idx : logger . warn ( \"Attempting annotation: did not select any data points.\" ) return example_old = self . dfs [ \"raw\" ] . at [ selected_idx [ 0 ], \"label\" ] self . dfs [ \"raw\" ] . at [ selected_idx , \"label\" ] = label example_new = self . dfs [ \"raw\" ] . at [ selected_idx [ 0 ], \"label\" ] logger . good ( f \"Updated DataFrame, e.g. { example_old } -> { example_new } \" ) self . _update_sources () self . plot () logger . good ( f \"Updated annotator plot at { current_time () } \" ) def callback_export ( event , path_root = None ): \"\"\" A callback on clicking the 'self.annotator_export' button. Saves the dataframe to a pickle. \"\"\" export_format = event . item # auto-determine the export path root if path_root is None : timestamp = current_time ( \"%Y%m %d %H%M%S\" ) path_root = f \"hover-annotated-df- { timestamp } \" if export_format == \"Excel\" : export_path = f \" { path_root } .xlsx\" self . dfs [ \"raw\" ] . to_excel ( export_path , index = False ) elif export_format == \"CSV\" : export_path = f \" { path_root } .csv\" self . dfs [ \"raw\" ] . to_csv ( export_path , index = False ) elif export_format == \"JSON\" : export_path = f \" { path_root } .json\" self . dfs [ \"raw\" ] . to_json ( export_path , orient = \"records\" ) elif export_format == \"pickle\" : export_path = f \" { path_root } .pkl\" self . dfs [ \"raw\" ] . to_pickle ( export_path ) else : raise ValueError ( f \"Unexpected export format { export_format } \" ) logger . good ( f \"Saved DataFrame to { export_path } \" ) # keep the references to the callbacks self . _callback_apply = callback_apply self . _callback_export = callback_export # assign callbacks self . annotator_apply . on_click ( self . _callback_apply ) self . annotator_export . on_click ( self . _callback_export ) plot ( self ) Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. Source code in hover/core/explorer.py def plot ( self ): \"\"\" Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. \"\"\" all_labels = sorted ( set ( self . dfs [ \"raw\" ][ \"label\" ] . values ), reverse = True ) cmap = auto_cmap ( all_labels ) self . figure . circle ( x = \"x\" , y = \"y\" , name = \"raw\" , color = factor_cmap ( \"label\" , cmap , all_labels ), legend_field = \"label\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ], ) hover.core.explorer.BokehSoftLabelExplorer Plot text data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios. __init__ ( self , df_dict , label_col , score_col , ** kwargs ) special On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". Source code in hover/core/explorer.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col kwargs . update ( { \"tooltips\" : bokeh_hover_tooltip ( label = True , text = True , image = False , coords = True , index = True , custom = { \"Soft Label\" : self . label_col , \"Soft Score\" : self . score_col }, ) } ) super () . __init__ ( df_dict , ** kwargs ) plot ( self , ** kwargs ) Plot the confidence map. Source code in hover/core/explorer.py def plot ( self , ** kwargs ): \"\"\" Plot the confidence map. \"\"\" # auto-detect all labels all_labels = set () for _key in self . __class__ . DATA_KEY_TO_KWARGS . keys (): _df = self . dfs [ _key ] _labels = set ( _df [ self . label_col ] . values ) all_labels = all_labels . union ( _labels ) all_labels = sorted ( all_labels , reverse = True ) cmap = auto_cmap ( all_labels ) for _key in self . __class__ . DATA_KEY_TO_KWARGS . keys (): # prepare plot settings preset_kwargs = { \"legend_field\" : self . label_col , \"color\" : factor_cmap ( self . label_col , cmap , all_labels ), \"fill_alpha\" : self . score_col , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( \"x\" , \"y\" , name = _key , source = self . sources [ _key ], ** eff_kwargs ) hover.core.explorer.BokehMarginExplorer Plot text data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios. __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ) special On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". Source code in hover/core/explorer.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs ) plot ( self , label , ** kwargs ) Plot the margins about a single label. Source code in hover/core/explorer.py def plot ( self , label , ** kwargs ): \"\"\" Plot the margins about a single label. \"\"\" # prepare plot settings axes = ( \"x\" , \"y\" ) eff_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ \"raw\" ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ \"raw\" ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ \"raw\" ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ \"raw\" ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )], ) increment_view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )], ) decrement_view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )], ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( * axes , name = \"raw\" , source = self . sources [ \"raw\" ], view = _view , ** eff_kwargs ) hover.core.explorer.BokehSnorkelExplorer Plot text data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set. __init__ ( self , df_dict , ** kwargs ) special On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". \"\"\" super () . __init__ ( df_dict , ** kwargs ) # initialize a list to keep track of plotted LFs self . lfs = [] self . palette = Category20 [ 20 ] _view_correct ( self , L_labeled ) private Determine the subset correctly labeled by a labeling function. Source code in hover/core/explorer.py def _view_correct ( self , L_labeled ): \"\"\" Determine the subset correctly labeled by a labeling function. \"\"\" agreed = self . dfs [ \"labeled\" ][ \"label\" ] . values == L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( agreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view _view_hit ( self , L_raw ) private Determine the subset hit by a labeling function. Source code in hover/core/explorer.py def _view_hit ( self , L_raw ): \"\"\" Determine the subset hit by a labeling function. \"\"\" indices = np . where ( L_raw != module_config . ABSTAIN_DECODED )[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( indices )]) return view _view_incorrect ( self , L_labeled ) private Determine the subset incorrectly labeled by a labeling function. Source code in hover/core/explorer.py def _view_incorrect ( self , L_labeled ): \"\"\" Determine the subset incorrectly labeled by a labeling function. \"\"\" disagreed = self . dfs [ \"labeled\" ][ \"label\" ] . values != L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( disagreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view _view_missed ( self , L_labeled , targets ) private Determine the subset missed by a labeling function. Source code in hover/core/explorer.py def _view_missed ( self , L_labeled , targets ): \"\"\" Determine the subset missed by a labeling function. \"\"\" targetable = np . isin ( self . dfs [ \"labeled\" ][ \"label\" ], targets ) abstained = L_labeled == module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( targetable , abstained ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( 'C' , 'I' , 'M' ), ** kwargs ) Plot a single labeling function. param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() param L_raw: labeling function predictions, in decoded labels, on the raw df. param L_labeled: labeling function predictions, in decoded labels, on the labeled df. param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer.py def plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" Plot a single labeling function. - param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() - param L_raw: labeling function predictions, in decoded labels, on the raw df. - param L_labeled: labeling function predictions, in decoded labels, on the labeled df. - param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # keep track of added LF self . lfs . append ( lf ) # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings axes = ( \"x\" , \"y\" ) legend_label = f \" { ', ' . join ( lf . targets ) } | { lf . name } \" color = self . palette [ len ( self . lfs ) - 1 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create correct/incorrect/missed/hit subsets to_plot = [] if \"C\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_correct ( L_labeled ), \"marker\" : self . figure . square , \"kwargs\" : labeled_glyph_kwargs , } ) if \"I\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_incorrect ( L_labeled ), \"marker\" : self . figure . x , \"kwargs\" : labeled_glyph_kwargs , } ) if \"M\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_missed ( L_labeled , lf . targets ), \"marker\" : self . figure . cross , \"kwargs\" : labeled_glyph_kwargs , } ) if \"H\" in include : to_plot . append ( { \"name\" : \"raw\" , \"view\" : self . _view_hit ( L_raw ), \"marker\" : self . figure . circle , \"kwargs\" : raw_glyph_kwargs , } ) # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _kwargs = _dict [ \"kwargs\" ] _marker ( * axes , source = _view . source , view = _view , ** _kwargs )","title":"hover.core.explorer"},{"location":"reference/core-explorer/#hovercoreexplorerbokehforlabeledtext","text":"","title":"hover.core.explorer.BokehForLabeledText"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText","text":"Base class that keeps template explorer settings. Assumes: in supplied dataframes (always) text data in a text column (always) xy coordinates in x and y columns (always) an index for the rows (likely) classification label in a label column Does not assume: what the explorer serves to do.","title":"hover.core.explorer.BokehForLabeledText"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText.__init__","text":"Operations shared by all child classes. settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults create widgets that child classes can override create data sources the correspond to class-specific data subsets. activate builtin search callbacks depending on the child class. create a (typically) blank figure under such settings Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" Operations shared by all child classes. - settle the figure settings by using child class defaults & kwargs overrides - settle the glyph settings by using child class defaults - create widgets that child classes can override - create data sources the correspond to class-specific data subsets. - activate builtin search callbacks depending on the child class. - create a (typically) blank figure under such settings \"\"\" logger . divider ( f \"Initializing { self . __class__ . __name__ } \" ) self . figure_kwargs = self . __class__ . DEFAULT_FIGURE_KWARGS . copy () self . figure_kwargs . update ( kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . DATA_KEY_TO_KWARGS . items () } self . _setup_widgets () self . _setup_dfs ( df_dict ) self . _setup_sources () self . _activate_search_builtin () self . figure = figure ( ** self . figure_kwargs ) self . reset_figure ()","title":"__init__()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText._activate_search_builtin","text":"Typically called once during initialization. Highlight positive search results and mute negative search results. Note that this is a template method which heavily depends on class attributes. Source code in hover/core/explorer.py def _activate_search_builtin ( self ): \"\"\" Typically called once during initialization. Highlight positive search results and mute negative search results. Note that this is a template method which heavily depends on class attributes. \"\"\" logger . info ( \"Activating built-in search\" ) for _key , _dict in self . __class__ . DATA_KEY_TO_KWARGS . items (): for _flag , _params in _dict [ \"search\" ] . items (): logger . info ( f \"Activated { _flag } on subset { _key } to respond to the search widgets.\" ) self . glyph_kwargs [ _key ] = self . activate_search ( self . sources [ _key ], self . glyph_kwargs [ _key ], altered_param = _params )","title":"_activate_search_builtin()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText._layout_widgets","text":"Define the layout of widgets. Source code in hover/core/explorer.py def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" return column ( self . search_pos , self . search_neg , self . data_key_button_group )","title":"_layout_widgets()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText._prelink_check","text":"Sanity check before linking two explorers. Source code in hover/core/explorer.py def _prelink_check ( self , other ): \"\"\" Sanity check before linking two explorers. \"\"\" assert other is not self , \"Self-loops are fordidden\" assert isinstance ( other , BokehForLabeledText ), \"Must link to BokehForLabelText\"","title":"_prelink_check()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText._setup_dfs","text":"Check and store DataFrames BY REFERENCE BY DEFAULT. Intended to be extended in child classes for pre/post processing. Source code in hover/core/explorer.py def _setup_dfs ( self , df_dict , copy = False ): \"\"\" Check and store DataFrames BY REFERENCE BY DEFAULT. Intended to be extended in child classes for pre/post processing. \"\"\" logger . info ( \"Setting up dfs\" ) supplied_keys = set ( df_dict . keys ()) expected_keys = set ( self . __class__ . DATA_KEY_TO_KWARGS . keys ()) supplied_not_expected = supplied_keys . difference ( expected_keys ) expected_not_supplied = expected_keys . difference ( supplied_keys ) for _key in supplied_not_expected : logger . warn ( f \" { self . __class__ . __name__ } .__init__(): got unexpected df key { _key } \" ) for _key in expected_not_supplied : logger . warn ( f \" { self . __class__ . __name__ } .__init__(): missing expected df key { _key } \" ) self . dfs = { _key : ( _df . copy () if copy else _df ) for _key , _df in df_dict . items () }","title":"_setup_dfs()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText._setup_sources","text":"Create (NOT UPDATE) ColumnDataSource objects. Intended to be extended in child classes for pre/post processing. Source code in hover/core/explorer.py def _setup_sources ( self ): \"\"\" Create (NOT UPDATE) ColumnDataSource objects. Intended to be extended in child classes for pre/post processing. \"\"\" logger . info ( \"Setting up sources\" ) self . sources = { _key : ColumnDataSource ( _df ) for _key , _df in self . dfs . items ()}","title":"_setup_sources()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText._setup_widgets","text":"Prepare widgets for interactive functionality. Create positive/negative text search boxes. Source code in hover/core/explorer.py def _setup_widgets ( self ): \"\"\" Prepare widgets for interactive functionality. Create positive/negative text search boxes. \"\"\" from bokeh.models import TextInput , CheckboxButtonGroup # set up text search widgets, without assigning callbacks yet # to provide more flexibility with callbacks logger . info ( \"Setting up widgets\" ) self . search_pos = TextInput ( title = \"Text contains (plain text, or /pattern/flag for regex):\" , width_policy = \"fit\" , height_policy = \"fit\" , ) self . search_neg = TextInput ( title = \"Text does not contain:\" , width_policy = \"fit\" , height_policy = \"fit\" ) # set up subset display toggles which do have clearly defined callbacks data_keys = list ( self . __class__ . DATA_KEY_TO_KWARGS . keys ()) self . data_key_button_group = CheckboxButtonGroup ( labels = data_keys , active = list ( range ( len ( data_keys ))) ) def update_data_key_display ( active ): visible_keys = { self . data_key_button_group . labels [ idx ] for idx in active } for _renderer in self . figure . renderers : # if the renderer has a name \"on the list\", update its visibility if _renderer . name in self . __class__ . DATA_KEY_TO_KWARGS . keys (): _renderer . visible = _renderer . name in visible_keys # store the callback (useful, for example, during automated tests) and link it self . update_data_key_display = update_data_key_display self . data_key_button_group . on_click ( self . update_data_key_display )","title":"_setup_widgets()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText._update_sources","text":"Update the sources with the corresponding dfs. Note that it seems mandatory to re-activate the search widgets. This is because the source loses plotting kwargs. Source code in hover/core/explorer.py def _update_sources ( self ): \"\"\" Update the sources with the corresponding dfs. Note that it seems mandatory to re-activate the search widgets. This is because the source loses plotting kwargs. \"\"\" for _key in self . __class__ . DATA_KEY_TO_KWARGS . keys (): self . sources [ _key ] . data = self . dfs [ _key ] self . _activate_search_builtin ()","title":"_update_sources()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText.activate_search","text":"Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Source code in hover/core/explorer.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. \"\"\" assert isinstance ( source , ColumnDataSource ) assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () param_key , param_pos , param_neg , param_default = altered_param num_points = len ( source . data [ \"text\" ]) default_param_list = [ param_default ] * num_points source . add ( default_param_list , f \" { param_key } \" ) updated_kwargs [ param_key ] = param_key search_callback = CustomJS ( args = { \"source\" : source , \"key_pos\" : self . search_pos , \"key_neg\" : self . search_neg , \"param_pos\" : param_pos , \"param_neg\" : param_neg , \"param_default\" : param_default , }, code = f \"\"\" const data = source.data; const text = data['text']; var arr = data[' { param_key } ']; \"\"\" + \"\"\" var search_pos = key_pos.value; var search_neg = key_neg.value; var valid_pos = (search_pos.length > 0); var valid_neg = (search_neg.length > 0); function determineAttr(candidate) { var score = 0; if (valid_pos) { if (candidate.search(search_pos) >= 0) { score += 1; } else { score -= 2; } }; if (valid_neg) { if (candidate.search(search_neg) < 0) { score += 1; } else { score -= 2; } }; if (score > 0) { return param_pos; } else if (score < 0) { return param_neg; } else {return param_default;} } function toRegex(search_key) { var match = search_key.match(new RegExp('^/(.*?)/([gimy]*)$')); if (match) { return new RegExp(match[1], match[2]); } else { return search_key; } } if (valid_pos) {search_pos = toRegex(search_pos);} if (valid_neg) {search_neg = toRegex(search_neg);} for (var i = 0; i < arr.length; i++) { arr[i] = determineAttr(text[i]); } source.change.emit() \"\"\" , ) self . search_pos . js_on_change ( \"value\" , search_callback ) self . search_neg . js_on_change ( \"value\" , search_callback ) return updated_kwargs","title":"activate_search()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText.from_dataset","text":"Construct from a SupervisableDataset. Source code in hover/core/explorer.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" Construct from a SupervisableDataset. \"\"\" assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} return cls ( df_dict , * args , ** kwargs )","title":"from_dataset()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText.link_selection","text":"Sync the selected indices between specified sources. Source code in hover/core/explorer.py def link_selection ( self , key , other , other_key ): \"\"\" Sync the selected indices between specified sources. \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] sl . selected . js_link ( \"indices\" , sr . selected , \"indices\" ) sr . selected . js_link ( \"indices\" , sl . selected , \"indices\" )","title":"link_selection()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText.link_xy_range","text":"Sync plotting ranges on the xy-plane. Source code in hover/core/explorer.py def link_xy_range ( self , other ): \"\"\" Sync plotting ranges on the xy-plane. \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr )","title":"link_xy_range()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText.reset_figure","text":"Start over on the figure. Source code in hover/core/explorer.py def reset_figure ( self ): \"\"\"Start over on the figure.\"\"\" logger . info ( \"Resetting figure\" ) self . figure . renderers . clear ()","title":"reset_figure()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehForLabeledText.view","text":"Define the layout of the whole explorer. Source code in hover/core/explorer.py def view ( self ): \"\"\"Define the layout of the whole explorer.\"\"\" return column ( self . _layout_widgets (), self . figure )","title":"view()"},{"location":"reference/core-explorer/#hovercoreexplorerbokehcorpusexplorer","text":"","title":"hover.core.explorer.BokehCorpusExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusExplorer","text":"Plot unlabeled, 2D-vectorized text data points in a corpus. Features: the search widgets will highlight the results through a change of color, which is arguably the best visual effect.","title":"hover.core.explorer.BokehCorpusExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusExplorer.__init__","text":"Requires the input dataframe to contain: (1) \"x\" and \"y\" columns for coordinates; (2) a \"text\" column for data point tooltips. Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" Requires the input dataframe to contain: (1) \"x\" and \"y\" columns for coordinates; (2) a \"text\" column for data point tooltips. \"\"\" super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusExplorer.plot","text":"(Re)-plot the corpus. Called just once per instance most of the time. Source code in hover/core/explorer.py def plot ( self , * args , ** kwargs ): \"\"\" (Re)-plot the corpus. Called just once per instance most of the time. \"\"\" self . figure . circle ( \"x\" , \"y\" , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] )","title":"plot()"},{"location":"reference/core-explorer/#hovercoreexplorerbokehcorpusannotator","text":"","title":"hover.core.explorer.BokehCorpusAnnotator"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusAnnotator","text":"Annoate text data points via callbacks. Features: alter values in the 'label' column through the widgets. SERVER ONLY : only works in a setting that allows Python callbacks.","title":"hover.core.explorer.BokehCorpusAnnotator"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusAnnotator.__init__","text":"Conceptually the same as the parent method. Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\"Conceptually the same as the parent method.\"\"\" super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusAnnotator._layout_widgets","text":"Define the layout of widgets. Source code in hover/core/explorer.py def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . search_pos , self . search_neg ), row ( self . data_key_button_group ), row ( self . annotator_input , self . annotator_apply , self . annotator_export ), ) return column ( * layout_rows )","title":"_layout_widgets()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusAnnotator._setup_widgets","text":"Create annotator widgets and assign Python callbacks. Source code in hover/core/explorer.py def _setup_widgets ( self ): \"\"\" Create annotator widgets and assign Python callbacks. \"\"\" from bokeh.models import TextInput , Button , Dropdown super () . _setup_widgets () self . annotator_input = TextInput ( title = \"Label:\" ) self . annotator_apply = Button ( label = \"Apply\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . annotator_export = Dropdown ( label = \"Export\" , button_type = \"warning\" , menu = [ \"Excel\" , \"CSV\" , \"JSON\" , \"pickle\" ], height_policy = \"fit\" , width_policy = \"min\" , ) def callback_apply (): \"\"\" A callback on clicking the 'self.annotator_apply' button. Update labels in the source. \"\"\" label = self . annotator_input . value selected_idx = self . sources [ \"raw\" ] . selected . indices if not selected_idx : logger . warn ( \"Attempting annotation: did not select any data points.\" ) return example_old = self . dfs [ \"raw\" ] . at [ selected_idx [ 0 ], \"label\" ] self . dfs [ \"raw\" ] . at [ selected_idx , \"label\" ] = label example_new = self . dfs [ \"raw\" ] . at [ selected_idx [ 0 ], \"label\" ] logger . good ( f \"Updated DataFrame, e.g. { example_old } -> { example_new } \" ) self . _update_sources () self . plot () logger . good ( f \"Updated annotator plot at { current_time () } \" ) def callback_export ( event , path_root = None ): \"\"\" A callback on clicking the 'self.annotator_export' button. Saves the dataframe to a pickle. \"\"\" export_format = event . item # auto-determine the export path root if path_root is None : timestamp = current_time ( \"%Y%m %d %H%M%S\" ) path_root = f \"hover-annotated-df- { timestamp } \" if export_format == \"Excel\" : export_path = f \" { path_root } .xlsx\" self . dfs [ \"raw\" ] . to_excel ( export_path , index = False ) elif export_format == \"CSV\" : export_path = f \" { path_root } .csv\" self . dfs [ \"raw\" ] . to_csv ( export_path , index = False ) elif export_format == \"JSON\" : export_path = f \" { path_root } .json\" self . dfs [ \"raw\" ] . to_json ( export_path , orient = \"records\" ) elif export_format == \"pickle\" : export_path = f \" { path_root } .pkl\" self . dfs [ \"raw\" ] . to_pickle ( export_path ) else : raise ValueError ( f \"Unexpected export format { export_format } \" ) logger . good ( f \"Saved DataFrame to { export_path } \" ) # keep the references to the callbacks self . _callback_apply = callback_apply self . _callback_export = callback_export # assign callbacks self . annotator_apply . on_click ( self . _callback_apply ) self . annotator_export . on_click ( self . _callback_export )","title":"_setup_widgets()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehCorpusAnnotator.plot","text":"Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. Source code in hover/core/explorer.py def plot ( self ): \"\"\" Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. \"\"\" all_labels = sorted ( set ( self . dfs [ \"raw\" ][ \"label\" ] . values ), reverse = True ) cmap = auto_cmap ( all_labels ) self . figure . circle ( x = \"x\" , y = \"y\" , name = \"raw\" , color = factor_cmap ( \"label\" , cmap , all_labels ), legend_field = \"label\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ], )","title":"plot()"},{"location":"reference/core-explorer/#hovercoreexplorerbokehsoftlabelexplorer","text":"","title":"hover.core.explorer.BokehSoftLabelExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSoftLabelExplorer","text":"Plot text data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios.","title":"hover.core.explorer.BokehSoftLabelExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSoftLabelExplorer.__init__","text":"On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". Source code in hover/core/explorer.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col kwargs . update ( { \"tooltips\" : bokeh_hover_tooltip ( label = True , text = True , image = False , coords = True , index = True , custom = { \"Soft Label\" : self . label_col , \"Soft Score\" : self . score_col }, ) } ) super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSoftLabelExplorer.plot","text":"Plot the confidence map. Source code in hover/core/explorer.py def plot ( self , ** kwargs ): \"\"\" Plot the confidence map. \"\"\" # auto-detect all labels all_labels = set () for _key in self . __class__ . DATA_KEY_TO_KWARGS . keys (): _df = self . dfs [ _key ] _labels = set ( _df [ self . label_col ] . values ) all_labels = all_labels . union ( _labels ) all_labels = sorted ( all_labels , reverse = True ) cmap = auto_cmap ( all_labels ) for _key in self . __class__ . DATA_KEY_TO_KWARGS . keys (): # prepare plot settings preset_kwargs = { \"legend_field\" : self . label_col , \"color\" : factor_cmap ( self . label_col , cmap , all_labels ), \"fill_alpha\" : self . score_col , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( \"x\" , \"y\" , name = _key , source = self . sources [ _key ], ** eff_kwargs )","title":"plot()"},{"location":"reference/core-explorer/#hovercoreexplorerbokehmarginexplorer","text":"","title":"hover.core.explorer.BokehMarginExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehMarginExplorer","text":"Plot text data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios.","title":"hover.core.explorer.BokehMarginExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehMarginExplorer.__init__","text":"On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". Source code in hover/core/explorer.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehMarginExplorer.plot","text":"Plot the margins about a single label. Source code in hover/core/explorer.py def plot ( self , label , ** kwargs ): \"\"\" Plot the margins about a single label. \"\"\" # prepare plot settings axes = ( \"x\" , \"y\" ) eff_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ \"raw\" ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ \"raw\" ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ \"raw\" ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ \"raw\" ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )], ) increment_view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )], ) decrement_view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )], ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( * axes , name = \"raw\" , source = self . sources [ \"raw\" ], view = _view , ** eff_kwargs )","title":"plot()"},{"location":"reference/core-explorer/#hovercoreexplorerbokehsnorkelexplorer","text":"","title":"hover.core.explorer.BokehSnorkelExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSnorkelExplorer","text":"Plot text data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set.","title":"hover.core.explorer.BokehSnorkelExplorer"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSnorkelExplorer.__init__","text":"On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". Source code in hover/core/explorer.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". \"\"\" super () . __init__ ( df_dict , ** kwargs ) # initialize a list to keep track of plotted LFs self . lfs = [] self . palette = Category20 [ 20 ]","title":"__init__()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSnorkelExplorer._view_correct","text":"Determine the subset correctly labeled by a labeling function. Source code in hover/core/explorer.py def _view_correct ( self , L_labeled ): \"\"\" Determine the subset correctly labeled by a labeling function. \"\"\" agreed = self . dfs [ \"labeled\" ][ \"label\" ] . values == L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( agreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view","title":"_view_correct()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSnorkelExplorer._view_hit","text":"Determine the subset hit by a labeling function. Source code in hover/core/explorer.py def _view_hit ( self , L_raw ): \"\"\" Determine the subset hit by a labeling function. \"\"\" indices = np . where ( L_raw != module_config . ABSTAIN_DECODED )[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( indices )]) return view","title":"_view_hit()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSnorkelExplorer._view_incorrect","text":"Determine the subset incorrectly labeled by a labeling function. Source code in hover/core/explorer.py def _view_incorrect ( self , L_labeled ): \"\"\" Determine the subset incorrectly labeled by a labeling function. \"\"\" disagreed = self . dfs [ \"labeled\" ][ \"label\" ] . values != L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( disagreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view","title":"_view_incorrect()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSnorkelExplorer._view_missed","text":"Determine the subset missed by a labeling function. Source code in hover/core/explorer.py def _view_missed ( self , L_labeled , targets ): \"\"\" Determine the subset missed by a labeling function. \"\"\" targetable = np . isin ( self . dfs [ \"labeled\" ][ \"label\" ], targets ) abstained = L_labeled == module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( targetable , abstained ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view","title":"_view_missed()"},{"location":"reference/core-explorer/#hover.core.explorer.BokehSnorkelExplorer.plot_lf","text":"Plot a single labeling function. param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() param L_raw: labeling function predictions, in decoded labels, on the raw df. param L_labeled: labeling function predictions, in decoded labels, on the labeled df. param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer.py def plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" Plot a single labeling function. - param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() - param L_raw: labeling function predictions, in decoded labels, on the raw df. - param L_labeled: labeling function predictions, in decoded labels, on the labeled df. - param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # keep track of added LF self . lfs . append ( lf ) # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings axes = ( \"x\" , \"y\" ) legend_label = f \" { ', ' . join ( lf . targets ) } | { lf . name } \" color = self . palette [ len ( self . lfs ) - 1 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create correct/incorrect/missed/hit subsets to_plot = [] if \"C\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_correct ( L_labeled ), \"marker\" : self . figure . square , \"kwargs\" : labeled_glyph_kwargs , } ) if \"I\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_incorrect ( L_labeled ), \"marker\" : self . figure . x , \"kwargs\" : labeled_glyph_kwargs , } ) if \"M\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_missed ( L_labeled , lf . targets ), \"marker\" : self . figure . cross , \"kwargs\" : labeled_glyph_kwargs , } ) if \"H\" in include : to_plot . append ( { \"name\" : \"raw\" , \"view\" : self . _view_hit ( L_raw ), \"marker\" : self . figure . circle , \"kwargs\" : raw_glyph_kwargs , } ) # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _kwargs = _dict [ \"kwargs\" ] _marker ( * axes , source = _view . source , view = _view , ** _kwargs )","title":"plot_lf()"},{"location":"reference/core-neural/","text":"hover.core.neural.VectorNet Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Please refer to hover.utils.torch_helper.VectorDataset and vector_dataloader for more info. __init__ ( self , vectorizer , architecture , state_dict_path , labels ) special param vectorizer(callable): a function that converts any string to a NumPy 1-D array. param architecture(class): a torch.nn.Module child class to be instantiated into a neural net. param state_dict_path(str): path to a PyTorch state dict that matches the architecture. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels ): \"\"\" - param vectorizer(callable): a function that converts any string to a NumPy 1-D array. - param architecture(class): a `torch.nn.Module` child class to be instantiated into a neural net. - param state_dict_path(str): path to a PyTorch state dict that matches the architecture. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" # set up label conversion self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) # set up vectorizer and the neural network with appropriate dimensions self . vectorizer = vectorizer vec_dim = self . vectorizer ( \"\" ) . shape [ 0 ] self . nn = architecture ( vec_dim , self . num_classes ) # if a state dict exists, load it and create a backup copy import os if os . path . isfile ( state_dict_path ): from shutil import copyfile try : self . nn . load_state_dict ( torch . load ( state_dict_path )) except Exception as e : logger . warn ( f \"Load VectorNet state path failed with { type ( e ) } : e\" ) state_dict_backup_path = ( f \" { state_dict_path } . { datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } \" ) copyfile ( state_dict_path , state_dict_backup_path ) # set a path to store updated parameters self . nn_update_path = state_dict_path # initialize an optimizer object and a dict to hold dynamic parameters self . nn_optimizer = torch . optim . Adam ( self . nn . parameters ()) self . _dynamic_params = { \"optimizer\" : { \"lr\" : 0.01 , \"betas\" : ( 0.9 , 0.999 )}} evaluate ( self , dev_loader , verbose = 1 ) Evaluate the neural network against a dev set. Source code in hover/core/neural.py def evaluate ( self , dev_loader , verbose = 1 ): \"\"\" Evaluate the neural network against a dev set. \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) logger . info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat predict_proba ( self , inps ) End-to-end single/multi-piece prediction from inp to class probabilities. Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" End-to-end single/multi-piece prediction from inp to class probabilities. \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = isinstance ( inps , str ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs save ( self , save_path = None ) Save the current state dict with authorization to overwrite. Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" Save the current state dict with authorization to overwrite. \"\"\" if save_path is None : save_path = self . nn_update_path torch . save ( self . nn . state_dict (), save_path ) train ( self , train_loader , dev_loader = None , epochs = 1 , verbose = 1 ) Train the neural network. This method is a vanilla template and is intended to be overridden in child classes. Also intended to be coupled with self.train_batch(). Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = 1 , verbose = 1 ): \"\"\" Train the neural network. - This method is a vanilla template and is intended to be overridden in child classes. - Also intended to be coupled with self.train_batch(). \"\"\" train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader , verbose = verbose ) if dev_loader is not None : acc , conf_mat = self . evaluate ( dev_loader , verbose = verbose ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info train_batch ( self , loaded_input , loaded_output , verbose = 1 ) Train the neural network for one batch. Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output , verbose = 1 ): \"\"\" Train the neural network for one batch. \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = cross_entropy_with_probs ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , ) train_epoch ( self , train_loader , * args , ** kwargs ) Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" Train the neural network for one epoch. - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs ) hover.core.neural.create_vector_net_from_module Create a TextVectorNet model, or of its child class. param specific_class(class): TextVectorNet or its child class. param model_module_name(str): path to a local Python module in the working directory whose init .py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py def create_vector_net_from_module ( specific_class , model_module_name , labels ): \"\"\" Create a TextVectorNet model, or of its child class. - param specific_class(class): TextVectorNet or its child class. - param model_module_name(str): path to a local Python module in the working directory whose __init__.py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" from importlib import import_module model_module = import_module ( model_module_name ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = specific_class ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ) return model","title":"hover.core.neural"},{"location":"reference/core-neural/#hovercoreneuralvectornet","text":"","title":"hover.core.neural.VectorNet"},{"location":"reference/core-neural/#hover.core.neural.VectorNet","text":"Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Please refer to hover.utils.torch_helper.VectorDataset and vector_dataloader for more info.","title":"hover.core.neural.VectorNet"},{"location":"reference/core-neural/#hover.core.neural.VectorNet.__init__","text":"param vectorizer(callable): a function that converts any string to a NumPy 1-D array. param architecture(class): a torch.nn.Module child class to be instantiated into a neural net. param state_dict_path(str): path to a PyTorch state dict that matches the architecture. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels ): \"\"\" - param vectorizer(callable): a function that converts any string to a NumPy 1-D array. - param architecture(class): a `torch.nn.Module` child class to be instantiated into a neural net. - param state_dict_path(str): path to a PyTorch state dict that matches the architecture. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" # set up label conversion self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) # set up vectorizer and the neural network with appropriate dimensions self . vectorizer = vectorizer vec_dim = self . vectorizer ( \"\" ) . shape [ 0 ] self . nn = architecture ( vec_dim , self . num_classes ) # if a state dict exists, load it and create a backup copy import os if os . path . isfile ( state_dict_path ): from shutil import copyfile try : self . nn . load_state_dict ( torch . load ( state_dict_path )) except Exception as e : logger . warn ( f \"Load VectorNet state path failed with { type ( e ) } : e\" ) state_dict_backup_path = ( f \" { state_dict_path } . { datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } \" ) copyfile ( state_dict_path , state_dict_backup_path ) # set a path to store updated parameters self . nn_update_path = state_dict_path # initialize an optimizer object and a dict to hold dynamic parameters self . nn_optimizer = torch . optim . Adam ( self . nn . parameters ()) self . _dynamic_params = { \"optimizer\" : { \"lr\" : 0.01 , \"betas\" : ( 0.9 , 0.999 )}}","title":"__init__()"},{"location":"reference/core-neural/#hover.core.neural.VectorNet.evaluate","text":"Evaluate the neural network against a dev set. Source code in hover/core/neural.py def evaluate ( self , dev_loader , verbose = 1 ): \"\"\" Evaluate the neural network against a dev set. \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) logger . info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat","title":"evaluate()"},{"location":"reference/core-neural/#hover.core.neural.VectorNet.predict_proba","text":"End-to-end single/multi-piece prediction from inp to class probabilities. Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" End-to-end single/multi-piece prediction from inp to class probabilities. \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = isinstance ( inps , str ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs","title":"predict_proba()"},{"location":"reference/core-neural/#hover.core.neural.VectorNet.save","text":"Save the current state dict with authorization to overwrite. Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" Save the current state dict with authorization to overwrite. \"\"\" if save_path is None : save_path = self . nn_update_path torch . save ( self . nn . state_dict (), save_path )","title":"save()"},{"location":"reference/core-neural/#hover.core.neural.VectorNet.train","text":"Train the neural network. This method is a vanilla template and is intended to be overridden in child classes. Also intended to be coupled with self.train_batch(). Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = 1 , verbose = 1 ): \"\"\" Train the neural network. - This method is a vanilla template and is intended to be overridden in child classes. - Also intended to be coupled with self.train_batch(). \"\"\" train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader , verbose = verbose ) if dev_loader is not None : acc , conf_mat = self . evaluate ( dev_loader , verbose = verbose ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info","title":"train()"},{"location":"reference/core-neural/#hover.core.neural.VectorNet.train_batch","text":"Train the neural network for one batch. Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output , verbose = 1 ): \"\"\" Train the neural network for one batch. \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = cross_entropy_with_probs ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , )","title":"train_batch()"},{"location":"reference/core-neural/#hover.core.neural.VectorNet.train_epoch","text":"Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" Train the neural network for one epoch. - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs )","title":"train_epoch()"},{"location":"reference/core-neural/#hovercoreneuralcreate_vector_net_from_module","text":"","title":"hover.core.neural.create_vector_net_from_module"},{"location":"reference/core-neural/#hover.core.neural.create_vector_net_from_module","text":"Create a TextVectorNet model, or of its child class. param specific_class(class): TextVectorNet or its child class. param model_module_name(str): path to a local Python module in the working directory whose init .py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py def create_vector_net_from_module ( specific_class , model_module_name , labels ): \"\"\" Create a TextVectorNet model, or of its child class. - param specific_class(class): TextVectorNet or its child class. - param model_module_name(str): path to a local Python module in the working directory whose __init__.py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" from importlib import import_module model_module = import_module ( model_module_name ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = specific_class ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ) return model","title":"hover.core.neural.create_vector_net_from_module"}]}