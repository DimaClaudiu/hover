{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hover - 0.4.0 Documentation This site hosts the tutorials and API references of the Hover package. Other resources beyond: the Binder repo","title":"Home"},{"location":"#hover-040-documentation","text":"This site hosts the tutorials and API references of the Hover package. Other resources beyond: the Binder repo","title":"Hover - 0.4.0 Documentation"},{"location":"pages/reference/core-dataset/","text":"Dataset objects which extend beyond DataFrames. Specifically, we need a collection of DataFrames where rows can be transferred cleanly and columns can be transformed easily. SupervisableDataset Docstring Feature-agnostic class for a dataset open to supervision. Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that the DataFrame form supports most kinds of operations; the list-of-dicts form could be useful for manipulations outside the scope of pandas; synchronization between the two forms should be called sparingly. __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = 'feature' , label_key = 'label' ) special Docstring Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. train_dictl: a list of dicts holding the train set. dev_dictl: a list of dicts holding the dev set. test_dictl: a list of dicts holding the test set. feature_key: key in each piece of dict mapping to the feature. label_key: key in each piece of dict mapping to the label in STRING form. Source code in hover/core/dataset.py def __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ info \"Docstring\" Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. - raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. - train_dictl: a list of dicts holding the train set. - dev_dictl: a list of dicts holding the dev set. - test_dictl: a list of dicts holding the test set. - feature_key: key in each piece of dict mapping to the feature. - label_key: key in each piece of dict mapping to the label in STRING form. \"\"\" self . _info ( \"Initializing...\" ) def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] self . dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } self . synchronize_dictl_to_df () self . df_deduplicate () self . synchronize_df_to_dictl () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _good ( \"Finished initialization.\" ) compute_2d_embedding ( self , vectorizer , method , ** kwargs ) Docstring Get embeddings in the xy-plane and return the reducer. Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ info \"Docstring\" Get embeddings in the xy-plane and return the reducer. \"\"\" from hover.core.representation.reduction import DimensionalityReducer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () trans_arr = np . array ([ vectorizer ( _inp ) for _inp in tqdm ( feature_inp )]) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] self . dfs [ _key ][ \"x\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 0 ] ) self . dfs [ _key ][ \"y\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 1 ] ) start_idx += _length return reducer copy ( self , use_df = True ) Docstring Create another instance, carrying over the data entries. Source code in hover/core/dataset.py def copy ( self , use_df = True ): \"\"\" ???+ info \"Docstring\" Create another instance, carrying over the data entries. \"\"\" if use_df : self . synchronize_df_to_dictl () return self . __class__ ( raw_dictl = self . dictls [ \"raw\" ], train_dictl = self . dictls [ \"train\" ], dev_dictl = self . dictls [ \"dev\" ], test_dictl = self . dictls [ \"test\" ], feature_key = self . __class__ . FEATURE_KEY , label_key = \"label\" , ) df_deduplicate ( self ) Docstring Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" ???+ info \"Docstring\" Cross-deduplicate data entries by feature between subsets. \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" ) loader ( self , key , vectorizer , batch_size = 64 , smoothing_coeff = 0.0 ) Docstring Prepare a Torch Dataloader for training or evaluation. key(str): the subset of dataset to use. vectorizer(callable): callable that turns a string into a vector. smoothing_coeff(float): the smoothing coeffient for soft labels. Source code in hover/core/dataset.py def loader ( self , key , vectorizer , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ info \"Docstring\" Prepare a Torch Dataloader for training or evaluation. - key(str): the subset of dataset to use. - vectorizer(callable): callable that turns a string into a vector. - smoothing_coeff(float): the smoothing coeffient for soft labels. \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import vector_dataloader , one_hot , label_smoothing # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () features = df [ self . __class__ . FEATURE_KEY ] . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) self . _info ( f \"Preparing { key } input vectors...\" ) input_vectors = [ vectorizer ( _f ) for _f in tqdm ( features )] if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) self . _info ( f \"Preparing { key } data loader...\" ) loader = vector_dataloader ( input_vectors , output_vectors , batch_size = batch_size ) self . _good ( f \"Prepared { key } loader consisting of { len ( features ) } examples with batch size { batch_size } \" ) return loader setup_label_coding ( self , verbose = True , debug = False ) Docstring Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. Source code in hover/core/dataset.py def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ info \"Docstring\" Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels () setup_pop_table ( self , ** kwargs ) Docstring Set up a table widget for subset data populations. Source code in hover/core/dataset.py def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ info \"Docstring\" Set up a table widget for subset data populations. \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] pop_data = dict ( label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Pop updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . data_committer . on_click ( update_population ) self . dedup_trigger . on_click ( update_population ) setup_widgets ( self ) Docstring Critical widgets for interactive data management. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" ???+ info \"Docstring\" Critical widgets for interactive data management. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. - PUSH shall be blocked until DEDUP is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. - COMMIT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget () subscribe_data_commit ( self , explorer , subset_mapping ) Docstring Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ info \"Docstring\" Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. \"\"\" def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return # take selected slice, ignoring ABSTAIN'ed rows sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" ) subscribe_update_push ( self , explorer , subset_mapping ) Docstring Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ info \"Docstring\" Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" # local import to avoid import cycles from hover.core.explorer.base import BokehBaseExplorer assert isinstance ( explorer , BokehBaseExplorer ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" ) synchronize_df_to_dictl ( self ) Docstring Re-make lists of dictionaries from dataframes. Source code in hover/core/dataset.py def synchronize_df_to_dictl ( self ): \"\"\" ???+ info \"Docstring\" Re-make lists of dictionaries from dataframes. \"\"\" self . dictls = dict () for _key , _df in self . dfs . items (): self . dictls [ _key ] = _df . to_dict ( orient = \"records\" ) synchronize_dictl_to_df ( self ) Docstring Re-make dataframes from lists of dictionaries. Source code in hover/core/dataset.py def synchronize_dictl_to_df ( self ): \"\"\" ???+ info \"Docstring\" Re-make dataframes from lists of dictionaries. \"\"\" self . dfs = dict () for _key , _dictl in self . dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df validate_labels ( self , raise_exception = True ) Docstring Check that every label is in the encoder. Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" ???+ info \"Docstring\" Check that every label is in the encoder. \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : x in self . label_encoder ) _invalid_indices = np . where ( _mask is False )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ({ self . dfs [ _key ] . loc [ _invalid_indices ]}) if raise_exception : raise ValueError ( \"invalid labels\" ) view ( self ) Docstring Defines the layout of bokeh models. Source code in hover/core/dataset.py def view ( self ): \"\"\" ???+ info \"Docstring\" Defines the layout of bokeh models. \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , row ( self . update_pusher , self . data_committer , self . dedup_trigger ), self . pop_table , ) SupervisableTextDataset Docstring Can add text-specific methods.","title":"hover.core.dataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset","text":"Dataset objects which extend beyond DataFrames. Specifically, we need a collection of DataFrames where rows can be transferred cleanly and columns can be transformed easily.","title":"hover.core.dataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset","text":"Docstring Feature-agnostic class for a dataset open to supervision. Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that the DataFrame form supports most kinds of operations; the list-of-dicts form could be useful for manipulations outside the scope of pandas; synchronization between the two forms should be called sparingly.","title":"SupervisableDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.__init__","text":"Docstring Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. train_dictl: a list of dicts holding the train set. dev_dictl: a list of dicts holding the dev set. test_dictl: a list of dicts holding the test set. feature_key: key in each piece of dict mapping to the feature. label_key: key in each piece of dict mapping to the label in STRING form. Source code in hover/core/dataset.py def __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ info \"Docstring\" Initialize the dataset with dictl and df forms; initialize the mapping between categorical-int and string labels. - raw_dictl: a list of dicts holding the raw data that DO NOT have annotation. - train_dictl: a list of dicts holding the train set. - dev_dictl: a list of dicts holding the dev set. - test_dictl: a list of dicts holding the test set. - feature_key: key in each piece of dict mapping to the feature. - label_key: key in each piece of dict mapping to the label in STRING form. \"\"\" self . _info ( \"Initializing...\" ) def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] self . dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } self . synchronize_dictl_to_df () self . df_deduplicate () self . synchronize_df_to_dictl () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _good ( \"Finished initialization.\" )","title":"__init__()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_2d_embedding","text":"Docstring Get embeddings in the xy-plane and return the reducer. Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ info \"Docstring\" Get embeddings in the xy-plane and return the reducer. \"\"\" from hover.core.representation.reduction import DimensionalityReducer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () trans_arr = np . array ([ vectorizer ( _inp ) for _inp in tqdm ( feature_inp )]) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] self . dfs [ _key ][ \"x\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 0 ] ) self . dfs [ _key ][ \"y\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 1 ] ) start_idx += _length return reducer","title":"compute_2d_embedding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.copy","text":"Docstring Create another instance, carrying over the data entries. Source code in hover/core/dataset.py def copy ( self , use_df = True ): \"\"\" ???+ info \"Docstring\" Create another instance, carrying over the data entries. \"\"\" if use_df : self . synchronize_df_to_dictl () return self . __class__ ( raw_dictl = self . dictls [ \"raw\" ], train_dictl = self . dictls [ \"train\" ], dev_dictl = self . dictls [ \"dev\" ], test_dictl = self . dictls [ \"test\" ], feature_key = self . __class__ . FEATURE_KEY , label_key = \"label\" , )","title":"copy()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.df_deduplicate","text":"Docstring Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" ???+ info \"Docstring\" Cross-deduplicate data entries by feature between subsets. \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" )","title":"df_deduplicate()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.loader","text":"Docstring Prepare a Torch Dataloader for training or evaluation. key(str): the subset of dataset to use. vectorizer(callable): callable that turns a string into a vector. smoothing_coeff(float): the smoothing coeffient for soft labels. Source code in hover/core/dataset.py def loader ( self , key , vectorizer , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ info \"Docstring\" Prepare a Torch Dataloader for training or evaluation. - key(str): the subset of dataset to use. - vectorizer(callable): callable that turns a string into a vector. - smoothing_coeff(float): the smoothing coeffient for soft labels. \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import vector_dataloader , one_hot , label_smoothing # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () features = df [ self . __class__ . FEATURE_KEY ] . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) self . _info ( f \"Preparing { key } input vectors...\" ) input_vectors = [ vectorizer ( _f ) for _f in tqdm ( features )] if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) self . _info ( f \"Preparing { key } data loader...\" ) loader = vector_dataloader ( input_vectors , output_vectors , batch_size = batch_size ) self . _good ( f \"Prepared { key } loader consisting of { len ( features ) } examples with batch size { batch_size } \" ) return loader","title":"loader()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_label_coding","text":"Docstring Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. Source code in hover/core/dataset.py def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ info \"Docstring\" Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add ABSTAIN as a no-label placeholder. \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels ()","title":"setup_label_coding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_pop_table","text":"Docstring Set up a table widget for subset data populations. Source code in hover/core/dataset.py def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ info \"Docstring\" Set up a table widget for subset data populations. \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] pop_data = dict ( label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Pop updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . data_committer . on_click ( update_population ) self . dedup_trigger . on_click ( update_population )","title":"setup_pop_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_widgets","text":"Docstring Critical widgets for interactive data management. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" ???+ info \"Docstring\" Critical widgets for interactive data management. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. - PUSH shall be blocked until DEDUP is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. - COMMIT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget ()","title":"setup_widgets()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_data_commit","text":"Docstring Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ info \"Docstring\" Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. \"\"\" def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return # take selected slice, ignoring ABSTAIN'ed rows sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" )","title":"subscribe_data_commit()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_update_push","text":"Docstring Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ info \"Docstring\" Enable pushing updated DataFrames to explorers that depend on them. Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" # local import to avoid import cycles from hover.core.explorer.base import BokehBaseExplorer assert isinstance ( explorer , BokehBaseExplorer ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" )","title":"subscribe_update_push()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_df_to_dictl","text":"Docstring Re-make lists of dictionaries from dataframes. Source code in hover/core/dataset.py def synchronize_df_to_dictl ( self ): \"\"\" ???+ info \"Docstring\" Re-make lists of dictionaries from dataframes. \"\"\" self . dictls = dict () for _key , _df in self . dfs . items (): self . dictls [ _key ] = _df . to_dict ( orient = \"records\" )","title":"synchronize_df_to_dictl()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_dictl_to_df","text":"Docstring Re-make dataframes from lists of dictionaries. Source code in hover/core/dataset.py def synchronize_dictl_to_df ( self ): \"\"\" ???+ info \"Docstring\" Re-make dataframes from lists of dictionaries. \"\"\" self . dfs = dict () for _key , _dictl in self . dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df","title":"synchronize_dictl_to_df()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.validate_labels","text":"Docstring Check that every label is in the encoder. Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" ???+ info \"Docstring\" Check that every label is in the encoder. \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : x in self . label_encoder ) _invalid_indices = np . where ( _mask is False )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ({ self . dfs [ _key ] . loc [ _invalid_indices ]}) if raise_exception : raise ValueError ( \"invalid labels\" )","title":"validate_labels()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.view","text":"Docstring Defines the layout of bokeh models. Source code in hover/core/dataset.py def view ( self ): \"\"\" ???+ info \"Docstring\" Defines the layout of bokeh models. \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , row ( self . update_pusher , self . data_committer , self . dedup_trigger ), self . pop_table , )","title":"view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableTextDataset","text":"Docstring Can add text-specific methods.","title":"SupervisableTextDataset"},{"location":"pages/reference/core-explorer/","text":"Interactive explorers mostly based on Bokeh. BokehAudioAnnotator The text flavor of BokehDataAnnotator. BokehAudioFinder The text flavor of BokehDataFinder. BokehAudioMargin The text flavor of BokehMarginExplorer. BokehAudioSnorkel The text flavor of BokehSnorkelExplorer. BokehAudioSoftLabel The text flavor of BokehSoftLabelExplorer. BokehImageAnnotator The text flavor of BokehDataAnnotator. BokehImageFinder The text flavor of BokehDataFinder. BokehImageMargin The text flavor of BokehMarginExplorer. BokehImageSnorkel The text flavor of BokehSnorkelExplorer. BokehImageSoftLabel The text flavor of BokehSoftLabelExplorer. BokehTextAnnotator The text flavor of BokehDataAnnotator. BokehTextFinder The text flavor of BokehDataFinder. BokehTextMargin The text flavor of BokehMarginExplorer. BokehTextSnorkel The text flavor of BokehSnorkelExplorer. BokehTextSoftLabel The text flavor of BokehSoftLabelExplorer. base Base class(es) for ALL explorer implementations. BokehBaseExplorer Base class for exploring data. Assumes: in supplied dataframes (always) xy coordinates in x and y columns; (always) an index for the rows; (always) classification label (or ABSTAIN) in a label column. Does not assume: a specific form of data; what the map serves to do. __init__ ( self , df_dict , ** kwargs ) special Operations shared by all child classes. settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults create widgets that child classes can override create data sources the correspond to class-specific data subsets. activate builtin search callbacks depending on the child class. create a (typically) blank figure under such settings Source code in hover/core/explorer/base.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" Operations shared by all child classes. - settle the figure settings by using child class defaults & kwargs overrides - settle the glyph settings by using child class defaults - create widgets that child classes can override - create data sources the correspond to class-specific data subsets. - activate builtin search callbacks depending on the child class. - create a (typically) blank figure under such settings \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip (), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_widgets () self . _setup_dfs ( df_dict ) self . _setup_sources () self . _activate_search_builtin () self . figure = figure ( ** self . figure_kwargs ) activate_search ( self , source , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Left to child classes that have a specific data format. Source code in hover/core/explorer/base.py @abstractmethod def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\"Left to child classes that have a specific data format.\"\"\" pass auto_labels_palette ( self ) Find all labels and an appropriate color map. Source code in hover/core/explorer/base.py def auto_labels_palette ( self ): \"\"\" Find all labels and an appropriate color map. \"\"\" labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) labels . discard ( module_config . ABSTAIN_DECODED ) labels = sorted ( labels , reverse = True ) assert len ( labels ) <= 20 , \"Too many labels to support (max at 20)\" palette = Category10 [ 10 ] if len ( labels ) <= 10 else Category20 [ 20 ] return labels , palette auto_legend ( method ) staticmethod Decorator that enforces the legend after each call of the decorated method. Source code in hover/core/explorer/base.py @staticmethod def auto_legend ( method ): \"\"\" Decorator that enforces the legend after each call of the decorated method. \"\"\" from functools import wraps @wraps ( method ) def wrapped ( ref , * args , ** kwargs ): if hasattr ( ref . figure , \"legend\" ): if hasattr ( ref . figure . legend , \"items\" ): ref . figure . legend . items . clear () retval = method ( ref , * args , ** kwargs ) ref . auto_legend_correction () return retval return wrapped auto_legend_correction ( self ) Find legend items and deduplicate by label, keeping the last glyph / legend item of each label. This is to resolve duplicate legend items due to automatic legend_group and incremental plotting. Source code in hover/core/explorer/base.py def auto_legend_correction ( self ): \"\"\" Find legend items and deduplicate by label, keeping the last glyph / legend item of each label. This is to resolve duplicate legend items due to automatic legend_group and incremental plotting. \"\"\" from collections import OrderedDict if not hasattr ( self . figure , \"legend\" ): self . _fail ( \"Attempting auto_legend_correction when there is no legend\" ) return # extract all items and start over items = self . figure . legend . items [:] self . figure . legend . items . clear () # use one item to hold all renderers matching its label label_to_item = OrderedDict () # deduplication for _item in items : _label = _item . label . get ( \"value\" , \"\" ) label_to_item [ _label ] = _item # WARNING: the current implementation discards renderer references. # This could be for the best because renderers add up their glyphs to the legend item. # To keep renderer references, see this example: # if _label not in label_to_item.keys(): # label_to_item[_label] = _item # else: # label_to_item[_label].renderers.extend(_item.renderers) self . figure . legend . items = list ( label_to_item . values ()) return from_dataset ( dataset , subset_mapping , * args , ** kwargs ) classmethod Construct from a SupervisableDataset. Source code in hover/core/explorer/base.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" Construct from a SupervisableDataset. \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} return cls ( df_dict , * args , ** kwargs ) link_selection ( self , key , other , other_key ) Sync the selected indices between specified sources. Source code in hover/core/explorer/base.py def link_selection ( self , key , other , other_key ): \"\"\" Sync the selected indices between specified sources. \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] sl . selected . js_link ( \"indices\" , sr . selected , \"indices\" ) sr . selected . js_link ( \"indices\" , sl . selected , \"indices\" ) link_xy_range ( self , other ) Sync plotting ranges on the xy-plane. Source code in hover/core/explorer/base.py def link_xy_range ( self , other ): \"\"\" Sync plotting ranges on the xy-plane. \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr ) plot ( self , * args , ** kwargs ) Plot something onto the figure. Source code in hover/core/explorer/base.py @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" Plot something onto the figure. \"\"\" pass view ( self ) Define the layout of the whole explorer. Source code in hover/core/explorer/base.py def view ( self ): \"\"\"Define the layout of the whole explorer.\"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure ) feature Intermediate classes based on the main feature. BokehForAudio Assumes on top of its parent class: in supplied dataframes (always) audio urls in an audio column Does not assume: what the explorer serves to do. activate_search ( self , source , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Trivial implementation until we decide how to search audios. Source code in hover/core/explorer/feature.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\"Trivial implementation until we decide how to search audios.\"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs BokehForImage Assumes on top of its parent class: in supplied dataframes (always) image urls in an image column Does not assume: what the explorer serves to do. activate_search ( self , source , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Trivial implementation until we decide how to search images. Source code in hover/core/explorer/feature.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\"Trivial implementation until we decide how to search images.\"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs BokehForText Assumes on top of its parent class: in supplied dataframes (always) text data in a text column Does not assume: what the explorer serves to do. activate_search ( self , source , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. Source code in hover/core/explorer/feature.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. \"\"\" assert isinstance ( source , ColumnDataSource ) assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () param_key , param_pos , param_neg , param_default = altered_param num_points = len ( source . data [ \"text\" ]) default_param_list = [ param_default ] * num_points source . add ( default_param_list , f \" { param_key } \" ) updated_kwargs [ param_key ] = param_key search_callback = CustomJS ( args = { \"source\" : source , \"key_pos\" : self . search_pos , \"key_neg\" : self . search_neg , \"param_pos\" : param_pos , \"param_neg\" : param_neg , \"param_default\" : param_default , }, code = f \"\"\" const data = source.data; const text = data['text']; var arr = data[' { param_key } ']; \"\"\" + \"\"\" var search_pos = key_pos.value; var search_neg = key_neg.value; var valid_pos = (search_pos.length > 0); var valid_neg = (search_neg.length > 0); function determineAttr(candidate) { var score = 0; if (valid_pos) { if (candidate.search(search_pos) >= 0) { score += 1; } else { score -= 2; } }; if (valid_neg) { if (candidate.search(search_neg) < 0) { score += 1; } else { score -= 2; } }; if (score > 0) { return param_pos; } else if (score < 0) { return param_neg; } else {return param_default;} } function toRegex(search_key) { var match = search_key.match(new RegExp('^/(.*?)/([gimy]*)$')); if (match) { return new RegExp(match[1], match[2]); } else { return search_key; } } if (valid_pos) {search_pos = toRegex(search_pos);} if (valid_neg) {search_neg = toRegex(search_neg);} for (var i = 0; i < arr.length; i++) { arr[i] = determineAttr(text[i]); } source.change.emit() \"\"\" , ) self . search_pos . js_on_change ( \"value\" , search_callback ) self . search_neg . js_on_change ( \"value\" , search_callback ) return updated_kwargs functionality Intermediate classes based on the functionality. BokehDataAnnotator Annoate data points via callbacks. Features: alter values in the 'label' column through the widgets. SERVER ONLY : only works in a setting that allows Python callbacks. plot ( self ) Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. Source code in hover/core/explorer/functionality.py @BokehBaseExplorer . auto_legend def plot ( self ): \"\"\" Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. \"\"\" labels , palette = self . auto_labels_palette () for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , color = factor_cmap ( \"label\" , palette = palette , factors = labels ), legend_group = \"label\" , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) BokehDataFinder Plot data points in grey ('gainsboro') and highlight search positives in coral. Features: the search widgets will highlight the results through a change of color, which is arguably the best visual effect. plot ( self , * args , ** kwargs ) Plot the data map. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\"Plot the data map.\"\"\" for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) BokehMarginExplorer Plot data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios. __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ) special On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs ) plot ( self , label , ** kwargs ) Plot the margins about a single label. Source code in hover/core/explorer/functionality.py def plot ( self , label , ** kwargs ): \"\"\" Plot the margins about a single label. \"\"\" for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( \"x\" , \"y\" , name = _key , source = _source , view = _view , ** eff_kwargs ) BokehSnorkelExplorer Plot data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set. __init__ ( self , df_dict , ** kwargs ) special On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". \"\"\" super () . __init__ ( df_dict , ** kwargs ) # initialize a list to keep track of plotted LFs self . lfs = [] self . palette = Category20 [ 20 ] plot ( self , * args , ** kwargs ) Overriding the parent method. Plot only the raw subset. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\" Overriding the parent method. Plot only the raw subset. \"\"\" self . figure . circle ( \"x\" , \"y\" , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" ) plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( 'C' , 'I' , 'M' ), ** kwargs ) Plot a single labeling function. param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() param L_raw: labeling function predictions, in decoded labels, on the raw df. param L_labeled: labeling function predictions, in decoded labels, on the labeled df. param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer/functionality.py def plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" Plot a single labeling function. - param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() - param L_raw: labeling function predictions, in decoded labels, on the raw df. - param L_labeled: labeling function predictions, in decoded labels, on the labeled df. - param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # keep track of added LF self . lfs . append ( lf ) # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings legend_label = f \" { ', ' . join ( lf . targets ) } | { lf . name } \" color = self . palette [ len ( self . lfs ) - 1 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create correct/incorrect/missed/hit subsets to_plot = [] if \"C\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_correct ( L_labeled ), \"marker\" : self . figure . square , \"kwargs\" : labeled_glyph_kwargs , } ) if \"I\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_incorrect ( L_labeled ), \"marker\" : self . figure . x , \"kwargs\" : labeled_glyph_kwargs , } ) if \"M\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_missed ( L_labeled , lf . targets ), \"marker\" : self . figure . cross , \"kwargs\" : labeled_glyph_kwargs , } ) if \"H\" in include : to_plot . append ( { \"name\" : \"raw\" , \"view\" : self . _view_hit ( L_raw ), \"marker\" : self . figure . circle , \"kwargs\" : raw_glyph_kwargs , } ) # plot created subsets for _dict in to_plot : _name = _dict [ \"name\" ] _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _kwargs = _dict [ \"kwargs\" ] _marker ( \"x\" , \"y\" , source = _view . source , view = _view , name = _name , ** _kwargs ) BokehSoftLabelExplorer Plot data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios. __init__ ( self , df_dict , label_col , score_col , ** kwargs ) special On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs ) plot ( self , ** kwargs ) Plot the confidence map. Source code in hover/core/explorer/functionality.py @BokehBaseExplorer . auto_legend def plot ( self , ** kwargs ): \"\"\" Plot the confidence map. \"\"\" labels , palette = self . auto_labels_palette () for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"legend_group\" : self . label_col , \"color\" : factor_cmap ( self . label_col , palette = palette , factors = labels ), \"fill_alpha\" : self . score_col , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) local_config bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None ) Create a Bokeh hover tooltip from a template. param label: whether to expect and show a \"label\" field. param text: whether to expect and show a \"text\" field. param image: whether to expect and show an \"image\" (url/path) field. param audio: whether to expect and show an \"audio\" (url/path) field. param coords: whether to show xy-coordinates. param index: whether to show indices in the dataset. param custom: {display: column} mapping of additional (text) tooltips. Source code in hover/core/explorer/local_config.py def bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None , ): \"\"\" Create a Bokeh hover tooltip from a template. - param label: whether to expect and show a \"label\" field. - param text: whether to expect and show a \"text\" field. - param image: whether to expect and show an \"image\" (url/path) field. - param audio: whether to expect and show an \"audio\" (url/path) field. - param coords: whether to show xy-coordinates. - param index: whether to show indices in the dataset. - param custom: {display: column} mapping of additional (text) tooltips. \"\"\" # initialize mutable default value custom = custom or dict () # prepare encapsulation of a div box and an associated script divbox_prefix = \"\"\"<div class=\"out tooltip\"> \\n \"\"\" divbox_suffix = \"\"\"</div> \\n \"\"\" script_prefix = \"\"\"<script> \\n \"\"\" script_suffix = \"\"\"</script> \\n \"\"\" # dynamically add contents to the div box and the script divbox = divbox_prefix script = script_prefix if label : divbox += \"\"\" <div> <span style=\"font-size: 16px; color: #966;\"> Label: @label </span> </div> \"\"\" if text : divbox += \"\"\" <div style=\"word-wrap: break-word; width: 95%; text-overflow: ellipsis; line-height: 90%\"> <span style=\"font-size: 11px;\"> Text: @text </span> </div> \"\"\" if image : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Image: @image </span> <img src=\"@image\" height=\"60\" alt=\"@image\" width=\"60\" style=\"float: left; margin: 0px 0px 0px 0px;\" border=\"2\" ></img> </div> \"\"\" if audio : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Audio: @audio </span> <audio autoplay preload=\"auto\" src=\"@audio\"> </audio> </div> \"\"\" if coords : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #060;\"> Coordinates: ($x, $y) </span> </div> \"\"\" if index : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #066;\"> Index: [$index] </span> </div> \"\"\" for _key , _field in custom . items (): divbox += f \"\"\" <div> <span style=\"font-size: 12px; color: #606;\"> { _key } : @ { _field } </span> </div> \"\"\" divbox += divbox_suffix script += script_suffix return divbox + script","title":"hover.core.explorer"},{"location":"pages/reference/core-explorer/#hover.core.explorer","text":"Interactive explorers mostly based on Bokeh.","title":"hover.core.explorer"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehAudioAnnotator","text":"The text flavor of BokehDataAnnotator.","title":"BokehAudioAnnotator"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehAudioFinder","text":"The text flavor of BokehDataFinder.","title":"BokehAudioFinder"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehAudioMargin","text":"The text flavor of BokehMarginExplorer.","title":"BokehAudioMargin"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehAudioSnorkel","text":"The text flavor of BokehSnorkelExplorer.","title":"BokehAudioSnorkel"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehAudioSoftLabel","text":"The text flavor of BokehSoftLabelExplorer.","title":"BokehAudioSoftLabel"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehImageAnnotator","text":"The text flavor of BokehDataAnnotator.","title":"BokehImageAnnotator"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehImageFinder","text":"The text flavor of BokehDataFinder.","title":"BokehImageFinder"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehImageMargin","text":"The text flavor of BokehMarginExplorer.","title":"BokehImageMargin"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehImageSnorkel","text":"The text flavor of BokehSnorkelExplorer.","title":"BokehImageSnorkel"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehImageSoftLabel","text":"The text flavor of BokehSoftLabelExplorer.","title":"BokehImageSoftLabel"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehTextAnnotator","text":"The text flavor of BokehDataAnnotator.","title":"BokehTextAnnotator"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehTextFinder","text":"The text flavor of BokehDataFinder.","title":"BokehTextFinder"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehTextMargin","text":"The text flavor of BokehMarginExplorer.","title":"BokehTextMargin"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehTextSnorkel","text":"The text flavor of BokehSnorkelExplorer.","title":"BokehTextSnorkel"},{"location":"pages/reference/core-explorer/#hover.core.explorer.BokehTextSoftLabel","text":"The text flavor of BokehSoftLabelExplorer.","title":"BokehTextSoftLabel"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base","text":"Base class(es) for ALL explorer implementations.","title":"base"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer","text":"Base class for exploring data. Assumes: in supplied dataframes (always) xy coordinates in x and y columns; (always) an index for the rows; (always) classification label (or ABSTAIN) in a label column. Does not assume: a specific form of data; what the map serves to do.","title":"BokehBaseExplorer"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.__init__","text":"Operations shared by all child classes. settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults create widgets that child classes can override create data sources the correspond to class-specific data subsets. activate builtin search callbacks depending on the child class. create a (typically) blank figure under such settings Source code in hover/core/explorer/base.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" Operations shared by all child classes. - settle the figure settings by using child class defaults & kwargs overrides - settle the glyph settings by using child class defaults - create widgets that child classes can override - create data sources the correspond to class-specific data subsets. - activate builtin search callbacks depending on the child class. - create a (typically) blank figure under such settings \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip (), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_widgets () self . _setup_dfs ( df_dict ) self . _setup_sources () self . _activate_search_builtin () self . figure = figure ( ** self . figure_kwargs )","title":"__init__()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.activate_search","text":"Left to child classes that have a specific data format. Source code in hover/core/explorer/base.py @abstractmethod def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\"Left to child classes that have a specific data format.\"\"\" pass","title":"activate_search()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.auto_labels_palette","text":"Find all labels and an appropriate color map. Source code in hover/core/explorer/base.py def auto_labels_palette ( self ): \"\"\" Find all labels and an appropriate color map. \"\"\" labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) labels . discard ( module_config . ABSTAIN_DECODED ) labels = sorted ( labels , reverse = True ) assert len ( labels ) <= 20 , \"Too many labels to support (max at 20)\" palette = Category10 [ 10 ] if len ( labels ) <= 10 else Category20 [ 20 ] return labels , palette","title":"auto_labels_palette()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.auto_legend","text":"Decorator that enforces the legend after each call of the decorated method. Source code in hover/core/explorer/base.py @staticmethod def auto_legend ( method ): \"\"\" Decorator that enforces the legend after each call of the decorated method. \"\"\" from functools import wraps @wraps ( method ) def wrapped ( ref , * args , ** kwargs ): if hasattr ( ref . figure , \"legend\" ): if hasattr ( ref . figure . legend , \"items\" ): ref . figure . legend . items . clear () retval = method ( ref , * args , ** kwargs ) ref . auto_legend_correction () return retval return wrapped","title":"auto_legend()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.auto_legend_correction","text":"Find legend items and deduplicate by label, keeping the last glyph / legend item of each label. This is to resolve duplicate legend items due to automatic legend_group and incremental plotting. Source code in hover/core/explorer/base.py def auto_legend_correction ( self ): \"\"\" Find legend items and deduplicate by label, keeping the last glyph / legend item of each label. This is to resolve duplicate legend items due to automatic legend_group and incremental plotting. \"\"\" from collections import OrderedDict if not hasattr ( self . figure , \"legend\" ): self . _fail ( \"Attempting auto_legend_correction when there is no legend\" ) return # extract all items and start over items = self . figure . legend . items [:] self . figure . legend . items . clear () # use one item to hold all renderers matching its label label_to_item = OrderedDict () # deduplication for _item in items : _label = _item . label . get ( \"value\" , \"\" ) label_to_item [ _label ] = _item # WARNING: the current implementation discards renderer references. # This could be for the best because renderers add up their glyphs to the legend item. # To keep renderer references, see this example: # if _label not in label_to_item.keys(): # label_to_item[_label] = _item # else: # label_to_item[_label].renderers.extend(_item.renderers) self . figure . legend . items = list ( label_to_item . values ()) return","title":"auto_legend_correction()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.from_dataset","text":"Construct from a SupervisableDataset. Source code in hover/core/explorer/base.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" Construct from a SupervisableDataset. \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} return cls ( df_dict , * args , ** kwargs )","title":"from_dataset()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.link_selection","text":"Sync the selected indices between specified sources. Source code in hover/core/explorer/base.py def link_selection ( self , key , other , other_key ): \"\"\" Sync the selected indices between specified sources. \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] sl . selected . js_link ( \"indices\" , sr . selected , \"indices\" ) sr . selected . js_link ( \"indices\" , sl . selected , \"indices\" )","title":"link_selection()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.link_xy_range","text":"Sync plotting ranges on the xy-plane. Source code in hover/core/explorer/base.py def link_xy_range ( self , other ): \"\"\" Sync plotting ranges on the xy-plane. \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr )","title":"link_xy_range()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.plot","text":"Plot something onto the figure. Source code in hover/core/explorer/base.py @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" Plot something onto the figure. \"\"\" pass","title":"plot()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.base.BokehBaseExplorer.view","text":"Define the layout of the whole explorer. Source code in hover/core/explorer/base.py def view ( self ): \"\"\"Define the layout of the whole explorer.\"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure )","title":"view()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.feature","text":"Intermediate classes based on the main feature.","title":"feature"},{"location":"pages/reference/core-explorer/#hover.core.explorer.feature.BokehForAudio","text":"Assumes on top of its parent class: in supplied dataframes (always) audio urls in an audio column Does not assume: what the explorer serves to do.","title":"BokehForAudio"},{"location":"pages/reference/core-explorer/#hover.core.explorer.feature.BokehForAudio.activate_search","text":"Trivial implementation until we decide how to search audios. Source code in hover/core/explorer/feature.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\"Trivial implementation until we decide how to search audios.\"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs","title":"activate_search()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.feature.BokehForImage","text":"Assumes on top of its parent class: in supplied dataframes (always) image urls in an image column Does not assume: what the explorer serves to do.","title":"BokehForImage"},{"location":"pages/reference/core-explorer/#hover.core.explorer.feature.BokehForImage.activate_search","text":"Trivial implementation until we decide how to search images. Source code in hover/core/explorer/feature.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\"Trivial implementation until we decide how to search images.\"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs","title":"activate_search()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.feature.BokehForText","text":"Assumes on top of its parent class: in supplied dataframes (always) text data in a text column Does not assume: what the explorer serves to do.","title":"BokehForText"},{"location":"pages/reference/core-explorer/#hover.core.explorer.feature.BokehForText.activate_search","text":"Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. Source code in hover/core/explorer/feature.py def activate_search ( self , source , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. \"\"\" assert isinstance ( source , ColumnDataSource ) assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () param_key , param_pos , param_neg , param_default = altered_param num_points = len ( source . data [ \"text\" ]) default_param_list = [ param_default ] * num_points source . add ( default_param_list , f \" { param_key } \" ) updated_kwargs [ param_key ] = param_key search_callback = CustomJS ( args = { \"source\" : source , \"key_pos\" : self . search_pos , \"key_neg\" : self . search_neg , \"param_pos\" : param_pos , \"param_neg\" : param_neg , \"param_default\" : param_default , }, code = f \"\"\" const data = source.data; const text = data['text']; var arr = data[' { param_key } ']; \"\"\" + \"\"\" var search_pos = key_pos.value; var search_neg = key_neg.value; var valid_pos = (search_pos.length > 0); var valid_neg = (search_neg.length > 0); function determineAttr(candidate) { var score = 0; if (valid_pos) { if (candidate.search(search_pos) >= 0) { score += 1; } else { score -= 2; } }; if (valid_neg) { if (candidate.search(search_neg) < 0) { score += 1; } else { score -= 2; } }; if (score > 0) { return param_pos; } else if (score < 0) { return param_neg; } else {return param_default;} } function toRegex(search_key) { var match = search_key.match(new RegExp('^/(.*?)/([gimy]*)$')); if (match) { return new RegExp(match[1], match[2]); } else { return search_key; } } if (valid_pos) {search_pos = toRegex(search_pos);} if (valid_neg) {search_neg = toRegex(search_neg);} for (var i = 0; i < arr.length; i++) { arr[i] = determineAttr(text[i]); } source.change.emit() \"\"\" , ) self . search_pos . js_on_change ( \"value\" , search_callback ) self . search_neg . js_on_change ( \"value\" , search_callback ) return updated_kwargs","title":"activate_search()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality","text":"Intermediate classes based on the functionality.","title":"functionality"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehDataAnnotator","text":"Annoate data points via callbacks. Features: alter values in the 'label' column through the widgets. SERVER ONLY : only works in a setting that allows Python callbacks.","title":"BokehDataAnnotator"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehDataAnnotator.plot","text":"Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. Source code in hover/core/explorer/functionality.py @BokehBaseExplorer . auto_legend def plot ( self ): \"\"\" Re-plot with the new labels. Overrides the parent method. Determines the label->color mapping dynamically. \"\"\" labels , palette = self . auto_labels_palette () for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , color = factor_cmap ( \"label\" , palette = palette , factors = labels ), legend_group = \"label\" , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehDataFinder","text":"Plot data points in grey ('gainsboro') and highlight search positives in coral. Features: the search widgets will highlight the results through a change of color, which is arguably the best visual effect.","title":"BokehDataFinder"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehDataFinder.plot","text":"Plot the data map. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\"Plot the data map.\"\"\" for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehMarginExplorer","text":"Plot data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios.","title":"BokehMarginExplorer"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehMarginExplorer.__init__","text":"On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col_a and label_col_b for \"label margins\". \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehMarginExplorer.plot","text":"Plot the margins about a single label. Source code in hover/core/explorer/functionality.py def plot ( self , label , ** kwargs ): \"\"\" Plot the margins about a single label. \"\"\" for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( \"x\" , \"y\" , name = _key , source = _source , view = _view , ** eff_kwargs )","title":"plot()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehSnorkelExplorer","text":"Plot data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set.","title":"BokehSnorkelExplorer"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehSnorkelExplorer.__init__","text":"On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" On top of the requirements of the parent class, the df_labeled input dataframe should contain: (1) a \"label\" column for \"ground truths\". \"\"\" super () . __init__ ( df_dict , ** kwargs ) # initialize a list to keep track of plotted LFs self . lfs = [] self . palette = Category20 [ 20 ]","title":"__init__()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot","text":"Overriding the parent method. Plot only the raw subset. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\" Overriding the parent method. Plot only the raw subset. \"\"\" self . figure . circle ( \"x\" , \"y\" , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot_lf","text":"Plot a single labeling function. param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() param L_raw: labeling function predictions, in decoded labels, on the raw df. param L_labeled: labeling function predictions, in decoded labels, on the labeled df. param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer/functionality.py def plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" Plot a single labeling function. - param lf: labeling function decorated by @hover.utils.snorkel_helper.labeling_function() - param L_raw: labeling function predictions, in decoded labels, on the raw df. - param L_labeled: labeling function predictions, in decoded labels, on the labeled df. - param include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # keep track of added LF self . lfs . append ( lf ) # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings legend_label = f \" { ', ' . join ( lf . targets ) } | { lf . name } \" color = self . palette [ len ( self . lfs ) - 1 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create correct/incorrect/missed/hit subsets to_plot = [] if \"C\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_correct ( L_labeled ), \"marker\" : self . figure . square , \"kwargs\" : labeled_glyph_kwargs , } ) if \"I\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_incorrect ( L_labeled ), \"marker\" : self . figure . x , \"kwargs\" : labeled_glyph_kwargs , } ) if \"M\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_missed ( L_labeled , lf . targets ), \"marker\" : self . figure . cross , \"kwargs\" : labeled_glyph_kwargs , } ) if \"H\" in include : to_plot . append ( { \"name\" : \"raw\" , \"view\" : self . _view_hit ( L_raw ), \"marker\" : self . figure . circle , \"kwargs\" : raw_glyph_kwargs , } ) # plot created subsets for _dict in to_plot : _name = _dict [ \"name\" ] _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _kwargs = _dict [ \"kwargs\" ] _marker ( \"x\" , \"y\" , source = _view . source , view = _view , name = _name , ** _kwargs )","title":"plot_lf()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehSoftLabelExplorer","text":"Plot data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios.","title":"BokehSoftLabelExplorer"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehSoftLabelExplorer.__init__","text":"On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" On top of the requirements of the parent class, the input dataframe should contain: (1) label_col and score_col for \"soft predictions\". \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.functionality.BokehSoftLabelExplorer.plot","text":"Plot the confidence map. Source code in hover/core/explorer/functionality.py @BokehBaseExplorer . auto_legend def plot ( self , ** kwargs ): \"\"\" Plot the confidence map. \"\"\" labels , palette = self . auto_labels_palette () for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"legend_group\" : self . label_col , \"color\" : factor_cmap ( self . label_col , palette = palette , factors = labels ), \"fill_alpha\" : self . score_col , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer/#hover.core.explorer.local_config","text":"","title":"local_config"},{"location":"pages/reference/core-explorer/#hover.core.explorer.local_config.bokeh_hover_tooltip","text":"Create a Bokeh hover tooltip from a template. param label: whether to expect and show a \"label\" field. param text: whether to expect and show a \"text\" field. param image: whether to expect and show an \"image\" (url/path) field. param audio: whether to expect and show an \"audio\" (url/path) field. param coords: whether to show xy-coordinates. param index: whether to show indices in the dataset. param custom: {display: column} mapping of additional (text) tooltips. Source code in hover/core/explorer/local_config.py def bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None , ): \"\"\" Create a Bokeh hover tooltip from a template. - param label: whether to expect and show a \"label\" field. - param text: whether to expect and show a \"text\" field. - param image: whether to expect and show an \"image\" (url/path) field. - param audio: whether to expect and show an \"audio\" (url/path) field. - param coords: whether to show xy-coordinates. - param index: whether to show indices in the dataset. - param custom: {display: column} mapping of additional (text) tooltips. \"\"\" # initialize mutable default value custom = custom or dict () # prepare encapsulation of a div box and an associated script divbox_prefix = \"\"\"<div class=\"out tooltip\"> \\n \"\"\" divbox_suffix = \"\"\"</div> \\n \"\"\" script_prefix = \"\"\"<script> \\n \"\"\" script_suffix = \"\"\"</script> \\n \"\"\" # dynamically add contents to the div box and the script divbox = divbox_prefix script = script_prefix if label : divbox += \"\"\" <div> <span style=\"font-size: 16px; color: #966;\"> Label: @label </span> </div> \"\"\" if text : divbox += \"\"\" <div style=\"word-wrap: break-word; width: 95%; text-overflow: ellipsis; line-height: 90%\"> <span style=\"font-size: 11px;\"> Text: @text </span> </div> \"\"\" if image : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Image: @image </span> <img src=\"@image\" height=\"60\" alt=\"@image\" width=\"60\" style=\"float: left; margin: 0px 0px 0px 0px;\" border=\"2\" ></img> </div> \"\"\" if audio : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Audio: @audio </span> <audio autoplay preload=\"auto\" src=\"@audio\"> </audio> </div> \"\"\" if coords : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #060;\"> Coordinates: ($x, $y) </span> </div> \"\"\" if index : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #066;\"> Index: [$index] </span> </div> \"\"\" for _key , _field in custom . items (): divbox += f \"\"\" <div> <span style=\"font-size: 12px; color: #606;\"> { _key } : @ { _field } </span> </div> \"\"\" divbox += divbox_suffix script += script_suffix return divbox + script","title":"bokeh_hover_tooltip()"},{"location":"pages/reference/core-neural/","text":"VectorNet Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Please refer to hover.utils.torch_helper.VectorDataset and vector_dataloader for more info. __init__ ( self , vectorizer , architecture , state_dict_path , labels ) special param vectorizer(callable): a function that converts any string to a NumPy 1-D array. param architecture(class): a torch.nn.Module child class to be instantiated into a neural net. param state_dict_path(str): path to a PyTorch state dict that matches the architecture. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels ): \"\"\" - param vectorizer(callable): a function that converts any string to a NumPy 1-D array. - param architecture(class): a `torch.nn.Module` child class to be instantiated into a neural net. - param state_dict_path(str): path to a PyTorch state dict that matches the architecture. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" # set up label conversion self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) # set up vectorizer and the neural network with appropriate dimensions self . vectorizer = vectorizer vec_dim = self . vectorizer ( \"\" ) . shape [ 0 ] self . nn = architecture ( vec_dim , self . num_classes ) # if a state dict exists, load it and create a backup copy import os if os . path . isfile ( state_dict_path ): from shutil import copyfile try : self . nn . load_state_dict ( torch . load ( state_dict_path )) except Exception as e : self . _warn ( f \"Load VectorNet state path failed with { type ( e ) } : e\" ) state_dict_backup_path = ( f \" { state_dict_path } . { datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } \" ) copyfile ( state_dict_path , state_dict_backup_path ) # set a path to store updated parameters self . nn_update_path = state_dict_path # initialize an optimizer object and a dict to hold dynamic parameters self . nn_optimizer = torch . optim . Adam ( self . nn . parameters ()) self . _dynamic_params = { \"optimizer\" : { \"lr\" : 0.01 , \"betas\" : ( 0.9 , 0.999 )}} adjust_optimizer_params ( self ) Dynamically change parameters of the neural net optimizer. Intended to be polymorphic in child classes and to be called per epoch. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" Dynamically change parameters of the neural net optimizer. - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ]) evaluate ( self , dev_loader , verbose = 1 ) Evaluate the neural network against a dev set. Source code in hover/core/neural.py def evaluate ( self , dev_loader , verbose = 1 ): \"\"\" Evaluate the neural network against a dev set. \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat from_module ( model_module , labels ) classmethod Create a VectorNet model, or of its child class. param model_module(module or str): (path to) a local Python module in the working directory whose init .py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py @classmethod def from_module ( cls , model_module , labels ): \"\"\" Create a VectorNet model, or of its child class. - param model_module(module or str): (path to) a local Python module in the working directory whose __init__.py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ) return model manifold_trajectory ( self , inps , method = 'umap' , ** kwargs ) TODO: need a clean way to pass kwargs to dimensionality reduction. vectorize inps forward propagate, keeping intermediates fit intermediates to 2D manifolds fit manifolds using Procrustes shape analysis fit shapes to trajectory splines param inps(list): input to calculate the manifold profile from. Source code in hover/core/neural.py def manifold_trajectory ( self , inps , method = \"umap\" , ** kwargs ): \"\"\" TODO: need a clean way to pass kwargs to dimensionality reduction. 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to 2D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines - param inps(list): input to calculate the manifold profile from. \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** kwargs ) return traj_arr , seq_arr , disparities predict_proba ( self , inps ) End-to-end single/multi-piece prediction from inp to class probabilities. Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" End-to-end single/multi-piece prediction from inp to class probabilities. \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = isinstance ( inps , str ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs save ( self , save_path = None ) Save the current state dict with authorization to overwrite. Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" Save the current state dict with authorization to overwrite. \"\"\" if save_path is None : save_path = self . nn_update_path torch . save ( self . nn . state_dict (), save_path ) train ( self , train_loader , dev_loader = None , epochs = 1 , verbose = 1 ) Train the neural network. This method is a vanilla template and is intended to be overridden in child classes. Also intended to be coupled with self.train_batch(). Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = 1 , verbose = 1 ): \"\"\" Train the neural network. - This method is a vanilla template and is intended to be overridden in child classes. - Also intended to be coupled with self.train_batch(). \"\"\" train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader , verbose = verbose ) if dev_loader is not None : acc , conf_mat = self . evaluate ( dev_loader , verbose = verbose ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info train_batch ( self , loaded_input , loaded_output , verbose = 1 ) Train the neural network for one batch. Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output , verbose = 1 ): \"\"\" Train the neural network for one batch. \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = cross_entropy_with_probs ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , ) train_epoch ( self , train_loader , * args , ** kwargs ) Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" Train the neural network for one epoch. - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs ) create_vector_net_from_module ( specific_class , model_module_name , labels ) Deprecated into a trivial invocation of VectorNet's class method. Source code in hover/core/neural.py @deprecated ( version = \"0.4.0\" , reason = \"will be removed in a future version; please use VectorNet.from_module() instead.\" , ) def create_vector_net_from_module ( specific_class , model_module_name , labels ): \"\"\" Deprecated into a trivial invocation of VectorNet's class method. \"\"\" return specific_class . from_module ( model_module_name , labels )","title":"hover.core.neural"},{"location":"pages/reference/core-neural/#hover.core.neural","text":"","title":"hover.core.neural"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet","text":"Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Please refer to hover.utils.torch_helper.VectorDataset and vector_dataloader for more info.","title":"VectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.__init__","text":"param vectorizer(callable): a function that converts any string to a NumPy 1-D array. param architecture(class): a torch.nn.Module child class to be instantiated into a neural net. param state_dict_path(str): path to a PyTorch state dict that matches the architecture. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels ): \"\"\" - param vectorizer(callable): a function that converts any string to a NumPy 1-D array. - param architecture(class): a `torch.nn.Module` child class to be instantiated into a neural net. - param state_dict_path(str): path to a PyTorch state dict that matches the architecture. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" # set up label conversion self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) # set up vectorizer and the neural network with appropriate dimensions self . vectorizer = vectorizer vec_dim = self . vectorizer ( \"\" ) . shape [ 0 ] self . nn = architecture ( vec_dim , self . num_classes ) # if a state dict exists, load it and create a backup copy import os if os . path . isfile ( state_dict_path ): from shutil import copyfile try : self . nn . load_state_dict ( torch . load ( state_dict_path )) except Exception as e : self . _warn ( f \"Load VectorNet state path failed with { type ( e ) } : e\" ) state_dict_backup_path = ( f \" { state_dict_path } . { datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } \" ) copyfile ( state_dict_path , state_dict_backup_path ) # set a path to store updated parameters self . nn_update_path = state_dict_path # initialize an optimizer object and a dict to hold dynamic parameters self . nn_optimizer = torch . optim . Adam ( self . nn . parameters ()) self . _dynamic_params = { \"optimizer\" : { \"lr\" : 0.01 , \"betas\" : ( 0.9 , 0.999 )}}","title":"__init__()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.adjust_optimizer_params","text":"Dynamically change parameters of the neural net optimizer. Intended to be polymorphic in child classes and to be called per epoch. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" Dynamically change parameters of the neural net optimizer. - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ])","title":"adjust_optimizer_params()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.evaluate","text":"Evaluate the neural network against a dev set. Source code in hover/core/neural.py def evaluate ( self , dev_loader , verbose = 1 ): \"\"\" Evaluate the neural network against a dev set. \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat","title":"evaluate()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.from_module","text":"Create a VectorNet model, or of its child class. param model_module(module or str): (path to) a local Python module in the working directory whose init .py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. Source code in hover/core/neural.py @classmethod def from_module ( cls , model_module , labels ): \"\"\" Create a VectorNet model, or of its child class. - param model_module(module or str): (path to) a local Python module in the working directory whose __init__.py file contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable. - param labels(list of str): the classification labels, e.g. [\"POSITIVE\", \"NEGATIVE\"]. \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ) return model","title":"from_module()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.manifold_trajectory","text":"TODO: need a clean way to pass kwargs to dimensionality reduction. vectorize inps forward propagate, keeping intermediates fit intermediates to 2D manifolds fit manifolds using Procrustes shape analysis fit shapes to trajectory splines param inps(list): input to calculate the manifold profile from. Source code in hover/core/neural.py def manifold_trajectory ( self , inps , method = \"umap\" , ** kwargs ): \"\"\" TODO: need a clean way to pass kwargs to dimensionality reduction. 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to 2D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines - param inps(list): input to calculate the manifold profile from. \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** kwargs ) return traj_arr , seq_arr , disparities","title":"manifold_trajectory()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.predict_proba","text":"End-to-end single/multi-piece prediction from inp to class probabilities. Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" End-to-end single/multi-piece prediction from inp to class probabilities. \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = isinstance ( inps , str ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs","title":"predict_proba()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.save","text":"Save the current state dict with authorization to overwrite. Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" Save the current state dict with authorization to overwrite. \"\"\" if save_path is None : save_path = self . nn_update_path torch . save ( self . nn . state_dict (), save_path )","title":"save()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train","text":"Train the neural network. This method is a vanilla template and is intended to be overridden in child classes. Also intended to be coupled with self.train_batch(). Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = 1 , verbose = 1 ): \"\"\" Train the neural network. - This method is a vanilla template and is intended to be overridden in child classes. - Also intended to be coupled with self.train_batch(). \"\"\" train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader , verbose = verbose ) if dev_loader is not None : acc , conf_mat = self . evaluate ( dev_loader , verbose = verbose ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info","title":"train()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_batch","text":"Train the neural network for one batch. Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output , verbose = 1 ): \"\"\" Train the neural network for one batch. \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = cross_entropy_with_probs ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , )","title":"train_batch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_epoch","text":"Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" Train the neural network for one epoch. - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs )","title":"train_epoch()"},{"location":"pages/reference/core-neural/#hover.core.neural.create_vector_net_from_module","text":"Deprecated into a trivial invocation of VectorNet's class method. Source code in hover/core/neural.py @deprecated ( version = \"0.4.0\" , reason = \"will be removed in a future version; please use VectorNet.from_module() instead.\" , ) def create_vector_net_from_module ( specific_class , model_module_name , labels ): \"\"\" Deprecated into a trivial invocation of VectorNet's class method. \"\"\" return specific_class . from_module ( model_module_name , labels )","title":"create_vector_net_from_module()"},{"location":"pages/reference/recipes/","text":"hover.recipes hover.recipes.stable Stable recipes whose function signatures should almost never change in the future. linked_annotator ( dataset , height = 600 , width = 600 ) Leveraging CorpusFinder which has the best search highlights. Layout: sidebar | [search here] | [annotate here] Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , height = 600 , width = 600 ): \"\"\" Leveraging CorpusFinder which has the best search highlights. Layout: sidebar | [search here] | [annotate here] \"\"\" finder = standard_finder ( dataset , height = height , width = width ) annotator = standard_annotator ( dataset , height = height , width = width ) # link coordinates and selections finder . link_xy_range ( annotator ) finder . link_selection ( \"raw\" , annotator , \"raw\" ) sidebar = dataset . view () layout = row ( sidebar , finder . view (), annotator . view ()) return layout simple_annotator ( dataset , height = 600 , width = 600 ) The most basic recipe, which nonetheless can be useful with decent 2-d embedding. Layout: sidebar | [annotate here] Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , height = 600 , width = 600 ): \"\"\" The most basic recipe, which nonetheless can be useful with decent 2-d embedding. Layout: sidebar | [annotate here] \"\"\" annotator = standard_annotator ( dataset , height = height , width = width ) sidebar = dataset . view () layout = row ( sidebar , annotator . view ()) return layout hover.recipes.experimental Experimental recipes whose function signatures might change significantly in the future. Use with caution. active_learning ( dataset , vectorizer , vecnet_callback , height = 600 , width = 600 ) Place a VectorNet in the loop. Layout: sidebar | [inspect soft labels here] | [annotate here] | [search here] Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vectorizer , vecnet_callback , height = 600 , width = 600 ): \"\"\" Place a VectorNet in the loop. Layout: sidebar | [inspect soft labels here] | [annotate here] | [search here] \"\"\" # building-block subroutines softlabel = standard_softlabel ( dataset , height = height , width = width ) annotator = standard_annotator ( dataset , height = height , width = width ) finder = standard_finder ( dataset , height = height , width = width ) # link coordinates and selections softlabel . link_xy_range ( annotator ) softlabel . link_xy_range ( finder ) softlabel . link_selection ( \"raw\" , annotator , \"raw\" ) softlabel . link_selection ( \"raw\" , finder , \"raw\" ) # recipe-specific widget def setup_model_retrainer (): model_retrainer = Button ( label = \"Train model\" , button_type = \"primary\" ) epochs_slider = Slider ( start = 1 , end = 20 , value = 1 , step = 1 , title = \"# epochs\" ) def retrain_model (): \"\"\" Callback function. \"\"\" model_retrainer . disabled = True logger . info ( \"Start training... button will be disabled temporarily.\" ) dataset . setup_label_coding () model = vecnet_callback ( dataset , vectorizer ) train_loader = dataset . loader ( \"train\" , vectorizer , smoothing_coeff = 0.2 ) dev_loader = dataset . loader ( \"dev\" , vectorizer ) _ = model . train ( train_loader , dev_loader , epochs = epochs_slider . value ) model . save () logger . good ( \"-- 1/2: retrained model\" ) for _key in [ \"raw\" , \"train\" , \"dev\" ]: _probs = model . predict_proba ( dataset . dfs [ _key ][ \"text\" ] . tolist ()) _labels = [ dataset . label_decoder [ _val ] for _val in _probs . argmax ( axis =- 1 ) ] _scores = _probs . max ( axis =- 1 ) . tolist () dataset . dfs [ _key ][ \"pred_label\" ] = pd . Series ( _labels ) dataset . dfs [ _key ][ \"pred_score\" ] = pd . Series ( _scores ) softlabel . _update_sources () softlabel . plot () model_retrainer . disabled = False logger . good ( \"-- 2/2: updated predictions. Training button is re-enabled.\" ) model_retrainer . on_click ( retrain_model ) return model_retrainer , epochs_slider model_retrainer , epochs_slider = setup_model_retrainer () sidebar = column ( model_retrainer , epochs_slider , dataset . view ()) layout = row ( sidebar , * [ _plot . view () for _plot in [ softlabel , annotator , finder ]]) return layout snorkel_crosscheck ( dataset , lf_list , height = 600 , width = 600 ) Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. Layout: sidebar | [inspect LFs here] | [annotate here] Source code in hover/recipes/experimental.py @servable ( title = \"Snorkel Crosscheck\" ) def snorkel_crosscheck ( dataset , lf_list , height = 600 , width = 600 ): \"\"\" Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. Layout: sidebar | [inspect LFs here] | [annotate here] \"\"\" # building-block subroutines snorkel = standard_snorkel ( dataset , height = height , width = width ) annotator = standard_annotator ( dataset , height = height , width = width ) # plot labeling functions for _lf in lf_list : snorkel . plot_lf ( _lf ) snorkel . figure . legend . click_policy = \"hide\" # link coordinates and selections snorkel . link_xy_range ( annotator ) snorkel . link_selection ( \"raw\" , annotator , \"raw\" ) sidebar = dataset . view () layout = row ( sidebar , snorkel . view (), annotator . view ()) return layout hover.recipes.subroutine Functions commonly used by classes in this submodule. Note that functions which are also used outside this submodule should be moved up. standard_annotator ( dataset , ** kwargs ) Standard TextAnnotator and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_annotator ( dataset , ** kwargs ): \"\"\" Standard TextAnnotator and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"annotator\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () annotator = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Annotator: apply labels to the selected points\" , ** kwargs , ) annotator . plot () # subscribe for df updates dataset . subscribe_update_push ( annotator , { _k : _k for _k in subsets }) # annotators can commit to a dataset dataset . subscribe_data_commit ( annotator , { \"raw\" : \"raw\" }) return annotator standard_finder ( dataset , ** kwargs ) Standard TextFinder and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_finder ( dataset , ** kwargs ): \"\"\" Standard TextFinder and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"finder\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () finder = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Text: use the search widget for highlights\" , ** kwargs , ) finder . plot () # subscribe for df updates dataset . subscribe_update_push ( finder , { _k : _k for _k in subsets }) return finder standard_snorkel ( dataset , ** kwargs ) Standard SnorkelExplorer and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_snorkel ( dataset , ** kwargs ): \"\"\" Standard SnorkelExplorer and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"snorkel\" , feature ) # first \"static\" version of the plot snorkel = explorer_cls . from_dataset ( dataset , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }, title = \"Snorkel: square for correct, x for incorrect, + for missed, o for hit; click on legends to hide or show LF\" , ** kwargs , ) snorkel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( snorkel , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }) return snorkel standard_softlabel ( dataset , ** kwargs ) Standard SoftLabelExplorer and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_softlabel ( dataset , ** kwargs ): \"\"\" Standard SoftLabelExplorer and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"softlabel\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () softlabel = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, \"pred_label\" , \"pred_score\" , title = \"Prediction Visualizer: retrain model and locate confusions\" , ** kwargs , ) softlabel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( softlabel , { _k : _k for _k in subsets }) return softlabel","title":"hover.recipes"},{"location":"pages/reference/recipes/#hoverrecipes","text":"","title":"hover.recipes"},{"location":"pages/reference/recipes/#hover.recipes.stable","text":"Stable recipes whose function signatures should almost never change in the future.","title":"stable"},{"location":"pages/reference/recipes/#hover.recipes.stable.linked_annotator","text":"Leveraging CorpusFinder which has the best search highlights. Layout: sidebar | [search here] | [annotate here] Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , height = 600 , width = 600 ): \"\"\" Leveraging CorpusFinder which has the best search highlights. Layout: sidebar | [search here] | [annotate here] \"\"\" finder = standard_finder ( dataset , height = height , width = width ) annotator = standard_annotator ( dataset , height = height , width = width ) # link coordinates and selections finder . link_xy_range ( annotator ) finder . link_selection ( \"raw\" , annotator , \"raw\" ) sidebar = dataset . view () layout = row ( sidebar , finder . view (), annotator . view ()) return layout","title":"linked_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.stable.simple_annotator","text":"The most basic recipe, which nonetheless can be useful with decent 2-d embedding. Layout: sidebar | [annotate here] Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , height = 600 , width = 600 ): \"\"\" The most basic recipe, which nonetheless can be useful with decent 2-d embedding. Layout: sidebar | [annotate here] \"\"\" annotator = standard_annotator ( dataset , height = height , width = width ) sidebar = dataset . view () layout = row ( sidebar , annotator . view ()) return layout","title":"simple_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.experimental","text":"Experimental recipes whose function signatures might change significantly in the future. Use with caution.","title":"experimental"},{"location":"pages/reference/recipes/#hover.recipes.experimental.active_learning","text":"Place a VectorNet in the loop. Layout: sidebar | [inspect soft labels here] | [annotate here] | [search here] Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vectorizer , vecnet_callback , height = 600 , width = 600 ): \"\"\" Place a VectorNet in the loop. Layout: sidebar | [inspect soft labels here] | [annotate here] | [search here] \"\"\" # building-block subroutines softlabel = standard_softlabel ( dataset , height = height , width = width ) annotator = standard_annotator ( dataset , height = height , width = width ) finder = standard_finder ( dataset , height = height , width = width ) # link coordinates and selections softlabel . link_xy_range ( annotator ) softlabel . link_xy_range ( finder ) softlabel . link_selection ( \"raw\" , annotator , \"raw\" ) softlabel . link_selection ( \"raw\" , finder , \"raw\" ) # recipe-specific widget def setup_model_retrainer (): model_retrainer = Button ( label = \"Train model\" , button_type = \"primary\" ) epochs_slider = Slider ( start = 1 , end = 20 , value = 1 , step = 1 , title = \"# epochs\" ) def retrain_model (): \"\"\" Callback function. \"\"\" model_retrainer . disabled = True logger . info ( \"Start training... button will be disabled temporarily.\" ) dataset . setup_label_coding () model = vecnet_callback ( dataset , vectorizer ) train_loader = dataset . loader ( \"train\" , vectorizer , smoothing_coeff = 0.2 ) dev_loader = dataset . loader ( \"dev\" , vectorizer ) _ = model . train ( train_loader , dev_loader , epochs = epochs_slider . value ) model . save () logger . good ( \"-- 1/2: retrained model\" ) for _key in [ \"raw\" , \"train\" , \"dev\" ]: _probs = model . predict_proba ( dataset . dfs [ _key ][ \"text\" ] . tolist ()) _labels = [ dataset . label_decoder [ _val ] for _val in _probs . argmax ( axis =- 1 ) ] _scores = _probs . max ( axis =- 1 ) . tolist () dataset . dfs [ _key ][ \"pred_label\" ] = pd . Series ( _labels ) dataset . dfs [ _key ][ \"pred_score\" ] = pd . Series ( _scores ) softlabel . _update_sources () softlabel . plot () model_retrainer . disabled = False logger . good ( \"-- 2/2: updated predictions. Training button is re-enabled.\" ) model_retrainer . on_click ( retrain_model ) return model_retrainer , epochs_slider model_retrainer , epochs_slider = setup_model_retrainer () sidebar = column ( model_retrainer , epochs_slider , dataset . view ()) layout = row ( sidebar , * [ _plot . view () for _plot in [ softlabel , annotator , finder ]]) return layout","title":"active_learning()"},{"location":"pages/reference/recipes/#hover.recipes.experimental.snorkel_crosscheck","text":"Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. Layout: sidebar | [inspect LFs here] | [annotate here] Source code in hover/recipes/experimental.py @servable ( title = \"Snorkel Crosscheck\" ) def snorkel_crosscheck ( dataset , lf_list , height = 600 , width = 600 ): \"\"\" Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. Layout: sidebar | [inspect LFs here] | [annotate here] \"\"\" # building-block subroutines snorkel = standard_snorkel ( dataset , height = height , width = width ) annotator = standard_annotator ( dataset , height = height , width = width ) # plot labeling functions for _lf in lf_list : snorkel . plot_lf ( _lf ) snorkel . figure . legend . click_policy = \"hide\" # link coordinates and selections snorkel . link_xy_range ( annotator ) snorkel . link_selection ( \"raw\" , annotator , \"raw\" ) sidebar = dataset . view () layout = row ( sidebar , snorkel . view (), annotator . view ()) return layout","title":"snorkel_crosscheck()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine","text":"Functions commonly used by classes in this submodule. Note that functions which are also used outside this submodule should be moved up.","title":"subroutine"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_annotator","text":"Standard TextAnnotator and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_annotator ( dataset , ** kwargs ): \"\"\" Standard TextAnnotator and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"annotator\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () annotator = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Annotator: apply labels to the selected points\" , ** kwargs , ) annotator . plot () # subscribe for df updates dataset . subscribe_update_push ( annotator , { _k : _k for _k in subsets }) # annotators can commit to a dataset dataset . subscribe_data_commit ( annotator , { \"raw\" : \"raw\" }) return annotator","title":"standard_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_finder","text":"Standard TextFinder and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_finder ( dataset , ** kwargs ): \"\"\" Standard TextFinder and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"finder\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () finder = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Text: use the search widget for highlights\" , ** kwargs , ) finder . plot () # subscribe for df updates dataset . subscribe_update_push ( finder , { _k : _k for _k in subsets }) return finder","title":"standard_finder()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_snorkel","text":"Standard SnorkelExplorer and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_snorkel ( dataset , ** kwargs ): \"\"\" Standard SnorkelExplorer and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"snorkel\" , feature ) # first \"static\" version of the plot snorkel = explorer_cls . from_dataset ( dataset , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }, title = \"Snorkel: square for correct, x for incorrect, + for missed, o for hit; click on legends to hide or show LF\" , ** kwargs , ) snorkel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( snorkel , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }) return snorkel","title":"standard_snorkel()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_softlabel","text":"Standard SoftLabelExplorer and its interaction with a dataset. Source code in hover/recipes/subroutine.py def standard_softlabel ( dataset , ** kwargs ): \"\"\" Standard SoftLabelExplorer and its interaction with a dataset. \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"softlabel\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () softlabel = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, \"pred_label\" , \"pred_score\" , title = \"Prediction Visualizer: retrain model and locate confusions\" , ** kwargs , ) softlabel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( softlabel , { _k : _k for _k in subsets }) return softlabel","title":"standard_softlabel()"},{"location":"pages/tutorial/dev-resources/","text":"Juniper for code blocks More CodeMirror themes Loading external CSS in HTML body Preloading binder with Juniper Loading BokehJS in JupyterLab","title":"Dev resources"},{"location":"pages/tutorial/t0-quickstart/","text":"Welcome to the minimal guide of hover ! Let's label some data and call it a day. @import url(\"../../../styles/monokai.css\"); @import url(\"../../../styles/juniper.css\"); Ingredient 1 / 3: Some Data Suppose that we have a list of data entries, each in the form of a dictionary: from hover.core.dataset import SupervisableTextDataset from faker import Faker import random # ---- fake data for illustation ---- fake_en = Faker(\"en\") def random_text(): return fake_en.paragraph(3) def random_raw_data(): return {\"content\": random_text()} def random_labeled_data(): return {\"content\": random_text(), \"mark\": random.choice([\"A\", \"B\"])} # ----------------------------------- dataset = SupervisableTextDataset( # raw data which do not have labels raw_dictl=[random_raw_data() for i in range(500)], # train / dev / test sets are optional train_dictl=[], dev_dictl=[random_labeled_data() for i in range(50)], test_dictl=[random_labeled_data() for i in range(50)], # adjust feature_key and label_key to your data feature_key=\"content\", label_key=\"mark\", ) # each subset is stored in its own DataFrame dataset.dfs[\"raw\"].head(5) Ingredient 2 / 3: Vectorizer To put our dataset sensibly on a 2-D \"map\", we will use a vectorizer for feature extraction, and then perform dimensionality reduction. Here's one way to define a vectorizer: import spacy import re nlp = spacy.load(\"en_core_web_md\") def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", text) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") Ingredient 3 / 3: Reduction The dataset has built-in high-level support for dimensionality reduction. Currently we can use umap or ivis . Optional dependencies The corresponding libraries do not ship with hover by default, and may need to be installed: for umap: pip install umap-learn for ivis: pip install ivis[cpu] or pip install ivis[gpu] umap-learn is installed in this demo environment. # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html dataset.compute_2d_embedding(vectorizer, \"umap\") # What we did adds 'x' and 'y' columns to the DataFrames in dataset.dfs # One could alternatively pre-compute these columns using any approach dataset.dfs[\"raw\"].head(5) Apply Labels Now we are ready to visualize and annotate! Basic tips When the cell below is executed, there should be in the output a visualization with a SupervisableDataset board on the left. an BokehDataAnnotator on the right. The SupervisableDataset comes with a few buttons: push : push Dataset updates to the bokeh plots. commit : add data entries selected in the Annotator to a specified subset. dedup : deduplicate across subsets (keep the last entry). The BokehDataAnnotator comes with a few buttons: raw / train / dev / test : choose which subsets to display. apply : apply the label input to the selected points in the raw subset only. export : save your data (all subsets) in a specified format. Best practices We've essentially put the data into neighborboods based on the vectorizer, but the quality, or the homogeneity of labels, of such neighborhoods can vary. hover over any data point to see its tooltip. take advantage of different selection tools to apply labels at appropriate scales. the search widget might turn out useful. note that it does not select points but highlights them. Known issue If you are running this code block on the documentation page, JupyterLab might fail to load/transmit BokehJS , and consequently the visualization might not show up. In that event, please run this tutorial locally. from hover.recipes import simple_annotator from bokeh.io import show, output_notebook # 'handle' is a function that renders elements in bokeh documents handle = simple_annotator(dataset) output_notebook() show(handle) new Juniper({ repo: 'phurwicz/hover-binder', isolateCells: false, theme: 'monokai' })","title":"Quickstart"},{"location":"pages/tutorial/t0-quickstart/#ingredient-1-3-some-data","text":"Suppose that we have a list of data entries, each in the form of a dictionary: from hover.core.dataset import SupervisableTextDataset from faker import Faker import random # ---- fake data for illustation ---- fake_en = Faker(\"en\") def random_text(): return fake_en.paragraph(3) def random_raw_data(): return {\"content\": random_text()} def random_labeled_data(): return {\"content\": random_text(), \"mark\": random.choice([\"A\", \"B\"])} # ----------------------------------- dataset = SupervisableTextDataset( # raw data which do not have labels raw_dictl=[random_raw_data() for i in range(500)], # train / dev / test sets are optional train_dictl=[], dev_dictl=[random_labeled_data() for i in range(50)], test_dictl=[random_labeled_data() for i in range(50)], # adjust feature_key and label_key to your data feature_key=\"content\", label_key=\"mark\", ) # each subset is stored in its own DataFrame dataset.dfs[\"raw\"].head(5)","title":"Ingredient 1 / 3: Some Data"},{"location":"pages/tutorial/t0-quickstart/#ingredient-2-3-vectorizer","text":"To put our dataset sensibly on a 2-D \"map\", we will use a vectorizer for feature extraction, and then perform dimensionality reduction. Here's one way to define a vectorizer: import spacy import re nlp = spacy.load(\"en_core_web_md\") def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", text) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\")","title":"Ingredient 2 / 3: Vectorizer"},{"location":"pages/tutorial/t0-quickstart/#ingredient-3-3-reduction","text":"The dataset has built-in high-level support for dimensionality reduction. Currently we can use umap or ivis . Optional dependencies The corresponding libraries do not ship with hover by default, and may need to be installed: for umap: pip install umap-learn for ivis: pip install ivis[cpu] or pip install ivis[gpu] umap-learn is installed in this demo environment. # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html dataset.compute_2d_embedding(vectorizer, \"umap\") # What we did adds 'x' and 'y' columns to the DataFrames in dataset.dfs # One could alternatively pre-compute these columns using any approach dataset.dfs[\"raw\"].head(5)","title":"Ingredient 3 / 3: Reduction"},{"location":"pages/tutorial/t0-quickstart/#apply-labels","text":"Now we are ready to visualize and annotate! Basic tips When the cell below is executed, there should be in the output a visualization with a SupervisableDataset board on the left. an BokehDataAnnotator on the right. The SupervisableDataset comes with a few buttons: push : push Dataset updates to the bokeh plots. commit : add data entries selected in the Annotator to a specified subset. dedup : deduplicate across subsets (keep the last entry). The BokehDataAnnotator comes with a few buttons: raw / train / dev / test : choose which subsets to display. apply : apply the label input to the selected points in the raw subset only. export : save your data (all subsets) in a specified format. Best practices We've essentially put the data into neighborboods based on the vectorizer, but the quality, or the homogeneity of labels, of such neighborhoods can vary. hover over any data point to see its tooltip. take advantage of different selection tools to apply labels at appropriate scales. the search widget might turn out useful. note that it does not select points but highlights them. Known issue If you are running this code block on the documentation page, JupyterLab might fail to load/transmit BokehJS , and consequently the visualization might not show up. In that event, please run this tutorial locally. from hover.recipes import simple_annotator from bokeh.io import show, output_notebook # 'handle' is a function that renders elements in bokeh documents handle = simple_annotator(dataset) output_notebook() show(handle) new Juniper({ repo: 'phurwicz/hover-binder', isolateCells: false, theme: 'monokai' })","title":"Apply Labels"},{"location":"pages/tutorial/t1-dataset/","text":"hover manages data through a SupervisableDataset class. Here we walk through some basic behaviors and interactions that can turn out useful. @import url(\"../../../styles/monokai.css\"); @import url(\"../../../styles/juniper.css\"); from hover.core.dataset import SupervisableTextDataset # ---- simplistic data for illustation ---- my_data = { \"raw\": [ {\"text\": \"Avocados are my favorite!\"}, {\"text\": \"Blueberries are not bad either.\"}, {\"text\": \"Citrus ... sure why not\"}, ], \"train\": [ {\"text\": \"Citrus ... sure why not\", \"label\": \"C\"}, {\"text\": \"Dragonfruits cost too much\", \"label\": \"D\"}, ], \"dev\": [ {\"text\": \"Dragonfruits cost too much\", \"label\": \"D\"}, {\"text\": \"Eggplants? Not in this scope.\", \"label\": \"E\"}, ], \"test\": [ {\"text\": \"Eggplants? Not in this scope.\", \"label\": \"E\"}, ] } # ----------------------------------- dataset = SupervisableTextDataset( raw_dictl=my_data[\"raw\"], train_dictl=my_data[\"train\"], dev_dictl=my_data[\"dev\"], test_dictl=my_data[\"test\"], # \"text\" is the default feature field for SupervisableTextDataset # \"label\" is the default label field for SupervisableDataset ) # Be aware of the automatic deduplication by feature # which keeps test > dev > train > raw dataset.dfs new Juniper({ repo: 'phurwicz/hover-binder', isolateCells: false, theme: 'monokai' })","title":"T1 dataset"}]}